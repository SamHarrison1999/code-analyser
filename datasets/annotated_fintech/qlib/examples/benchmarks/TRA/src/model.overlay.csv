annotation,annotation_tokens,confidence,end_token,label,line,reason,severity,start_token,text,tokens
üß† ML Signal: Conditional device selection for model training,"[8582, 100, 254, 10373, 26484, 25, 9724, 1859, 3335, 6356, 329, 2746, 3047]",0.5,0,ml_signal,16,Conditional device selection for model training,,0,,[]
üß† ML Signal: Use of model configuration and training configuration parameters,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 2746, 8398, 290, 3047, 8398, 10007]",1.0,0,ml_signal,18,Use of model configuration and training configuration parameters,,0,,[]
"üß† ML Signal: Default model type is set to ""LSTM""","[8582, 100, 254, 10373, 26484, 25, 15161, 2746, 2099, 318, 900, 284, 366, 43, 2257, 44, 1]",1.0,0,ml_signal,18,"Default model type is set to ""LSTM""",,0,,[]
"üß† ML Signal: Learning rate, number of epochs, and early stopping criteria are specified","[8582, 100, 254, 10373, 26484, 25, 18252, 2494, 11, 1271, 286, 36835, 82, 11, 290, 1903, 12225, 9987, 389, 7368]",0.5,0,ml_signal,18,"Learning rate, number of epochs, and early stopping criteria are specified",,0,,[]
üß† ML Signal: Use of random seed for reproducibility,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 4738, 9403, 329, 8186, 66, 2247]",1.0,0,ml_signal,18,Use of random seed for reproducibility,,0,,[]
‚ö†Ô∏è SAST Risk (Low): Use of eval() can lead to code execution vulnerabilities,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 5765, 286, 5418, 3419, 460, 1085, 284, 2438, 9706, 23805]",1.0,0,sast_risk,18,Use of eval() can lead to code execution vulnerabilities,Low,0,,[]
‚úÖ Best Practice: Use of logging for tracking model initialization and parameter counts,"[26486, 227, 6705, 19939, 25, 5765, 286, 18931, 329, 9646, 2746, 37588, 290, 11507, 9853]",0.5,0,best_practice,18,Use of logging for tracking model initialization and parameter counts,,0,,[]
‚ö†Ô∏è SAST Risk (Low): Loading model state from a file without validation,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 12320, 2746, 1181, 422, 257, 2393, 1231, 21201]",0.5,0,sast_risk,18,Loading model state from a file without validation,Low,0,,[]
‚úÖ Best Practice: Freezing model parameters for certain training scenarios,"[26486, 227, 6705, 19939, 25, 3232, 9510, 2746, 10007, 329, 1728, 3047, 13858]",0.5,0,best_practice,18,Freezing model parameters for certain training scenarios,,0,,[]
‚úÖ Best Practice: Logging the number of parameters in the model,"[26486, 227, 6705, 19939, 25, 5972, 2667, 262, 1271, 286, 10007, 287, 262, 2746]",0.5,0,best_practice,18,Logging the number of parameters in the model,,0,,[]
‚úÖ Best Practice: Logging the number of parameters in the TRA component,"[26486, 227, 6705, 19939, 25, 5972, 2667, 262, 1271, 286, 10007, 287, 262, 29125, 7515]",0.5,0,best_practice,18,Logging the number of parameters in the TRA component,,0,,[]
üß† ML Signal: Use of Adam optimizer with specified learning rate,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 7244, 6436, 7509, 351, 7368, 4673, 2494]",0.5,0,ml_signal,18,Use of Adam optimizer with specified learning rate,,0,,[]
üß† ML Signal: Storing configuration and training parameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 8398, 290, 3047, 10007, 355, 4554, 9633]",0.5,11,ml_signal,29,Storing configuration and training parameters as instance variables,,0,"        model_config,","[220, 220, 220, 220, 220, 220, 220, 2746, 62, 11250, 11]"
‚úÖ Best Practice: Warning about ignored `eval_train` when using TRA with multiple states,"[26486, 227, 6705, 19939, 25, 15932, 546, 9514, 4600, 18206, 62, 27432, 63, 618, 1262, 29125, 351, 3294, 2585]",0.5,22,best_practice,30,Warning about ignored `eval_train` when using TRA with multiple states,,11,"        tra_config,","[220, 220, 220, 220, 220, 220, 220, 1291, 62, 11250, 11]"
‚ö†Ô∏è SAST Risk (Low): Potential directory traversal if `logdir` is user-controlled,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 8619, 33038, 282, 611, 4600, 6404, 15908, 63, 318, 2836, 12, 14401]",0.5,37,sast_risk,31,Potential directory traversal if `logdir` is user-controlled,Low,22,"        model_type=""LSTM"",","[220, 220, 220, 220, 220, 220, 220, 2746, 62, 4906, 2625, 43, 2257, 44, 1600]"
‚úÖ Best Practice: Creating log directory if it does not exist,"[26486, 227, 6705, 19939, 25, 30481, 2604, 8619, 611, 340, 857, 407, 2152]",0.5,52,best_practice,32,Creating log directory if it does not exist,,37,"        lr=1e-3,","[220, 220, 220, 220, 220, 220, 220, 300, 81, 28, 16, 68, 12, 18, 11]"
üß† ML Signal: Tracking the fitted state and global step of the model,"[8582, 100, 254, 10373, 26484, 25, 37169, 262, 18235, 1181, 290, 3298, 2239, 286, 262, 2746]",0.5,67,ml_signal,33,Tracking the fitted state and global step of the model,,52,"        n_epochs=500,","[220, 220, 220, 220, 220, 220, 220, 299, 62, 538, 5374, 82, 28, 4059, 11]"
üß† ML Signal: Iterating over batches in a dataset is a common pattern in training loops,"[8582, 100, 254, 10373, 26484, 25, 40806, 803, 625, 37830, 287, 257, 27039, 318, 257, 2219, 3912, 287, 3047, 23607]",1.0,91,ml_signal,87,Iterating over batches in a dataset is a common pattern in training loops,,67,            if os.path.exists(self.logdir):,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 28686, 13, 6978, 13, 1069, 1023, 7, 944, 13, 6404, 15908, 2599]"
üß† ML Signal: Incrementing a global step counter is a common pattern in training loops,"[8582, 100, 254, 10373, 26484, 25, 10791, 434, 278, 257, 3298, 2239, 3753, 318, 257, 2219, 3912, 287, 3047, 23607]",1.0,106,ml_signal,92,Incrementing a global step counter is a common pattern in training loops,,91,        self.global_step = -1,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 20541, 62, 9662, 796, 532, 16]"
üß† ML Signal: Forward pass through the model,"[8582, 100, 254, 10373, 26484, 25, 19530, 1208, 832, 262, 2746]",0.5,106,ml_signal,97,Forward pass through the model,,106,,[]
üß† ML Signal: Forward pass through another model or layer,"[8582, 100, 254, 10373, 26484, 25, 19530, 1208, 832, 1194, 2746, 393, 7679]",0.5,106,ml_signal,99,Forward pass through another model or layer,,106,,[]
üß† ML Signal: Calculating loss using mean squared error,"[8582, 100, 254, 10373, 26484, 25, 27131, 803, 2994, 1262, 1612, 44345, 4049]",1.0,128,ml_signal,101,Calculating loss using mean squared error,,106,        if self.max_steps_per_epoch is not None:,"[220, 220, 220, 220, 220, 220, 220, 611, 2116, 13, 9806, 62, 20214, 62, 525, 62, 538, 5374, 318, 407, 6045, 25]"
‚ö†Ô∏è SAST Risk (Low): Using a fixed epsilon value in sinkhorn could lead to numerical stability issues,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 8554, 257, 5969, 304, 862, 33576, 1988, 287, 14595, 25311, 714, 1085, 284, 29052, 10159, 2428]",1.0,152,sast_risk,107,Using a fixed epsilon value in sinkhorn could lead to numerical stability issues,Low,128,"        for batch in tqdm(data_set, total=max_steps):","[220, 220, 220, 220, 220, 220, 220, 329, 15458, 287, 256, 80, 36020, 7, 7890, 62, 2617, 11, 2472, 28, 9806, 62, 20214, 2599]"
‚ö†Ô∏è SAST Risk (Medium): Potential for gradient explosion if loss is not properly managed,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 31205, 2599, 32480, 329, 31312, 11278, 611, 2994, 318, 407, 6105, 5257]",1.0,170,sast_risk,112,Potential for gradient explosion if loss is not properly managed,Medium,152,            self.global_step += 1,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 20541, 62, 9662, 15853, 352]"
‚úÖ Best Practice: Resetting gradients after optimizer step,"[26486, 227, 6705, 19939, 25, 30027, 889, 3915, 2334, 706, 6436, 7509, 2239]",0.5,170,best_practice,115,Resetting gradients after optimizer step,,170,,[]
"üß† ML Signal: Model evaluation mode is set, indicating a testing phase","[8582, 100, 254, 10373, 26484, 25, 9104, 12660, 4235, 318, 900, 11, 12739, 257, 4856, 7108]",1.0,170,ml_signal,113,"Model evaluation mode is set, indicating a testing phase",,170,,[]
"üß† ML Signal: Transition model evaluation mode is set, indicating a testing phase","[8582, 100, 254, 10373, 26484, 25, 40658, 2746, 12660, 4235, 318, 900, 11, 12739, 257, 4856, 7108]",1.0,170,ml_signal,115,"Transition model evaluation mode is set, indicating a testing phase",,170,,[]
"üß† ML Signal: Dataset evaluation mode is set, indicating a testing phase","[8582, 100, 254, 10373, 26484, 25, 16092, 292, 316, 12660, 4235, 318, 900, 11, 12739, 257, 4856, 7108]",1.0,207,ml_signal,117,"Dataset evaluation mode is set, indicating a testing phase",,170,"            hist_loss = data[:, : -data_set.horizon, -self.tra.num_states :]","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1554, 62, 22462, 796, 1366, 58, 45299, 1058, 532, 7890, 62, 2617, 13, 17899, 8637, 11, 532, 944, 13, 9535, 13, 22510, 62, 27219, 1058, 60]"
"üß† ML Signal: Iterating over dataset batches, common in model evaluation","[8582, 100, 254, 10373, 26484, 25, 40806, 803, 625, 27039, 37830, 11, 2219, 287, 2746, 12660]",1.0,207,ml_signal,121,"Iterating over dataset batches, common in model evaluation",,207,,[]
"‚ö†Ô∏è SAST Risk (Low): No gradient tracking, safe for inference","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 1400, 31312, 9646, 11, 3338, 329, 32278]",0.5,207,sast_risk,126,"No gradient tracking, safe for inference",Low,207,,[]
"üß† ML Signal: Assigning computed loss to dataset, possibly for further analysis","[8582, 100, 254, 10373, 26484, 25, 2195, 38944, 29231, 2994, 284, 27039, 11, 5457, 329, 2252, 3781]",0.5,241,ml_signal,132,"Assigning computed loss to dataset, possibly for further analysis",,207,                reg = prob.log().mul(P).sum(dim=-1).mean(),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 842, 796, 1861, 13, 6404, 22446, 76, 377, 7, 47, 737, 16345, 7, 27740, 10779, 16, 737, 32604, 3419]"
"üß† ML Signal: Creating a DataFrame for predictions, common in result analysis","[8582, 100, 254, 10373, 26484, 25, 30481, 257, 6060, 19778, 329, 16277, 11, 2219, 287, 1255, 3781]",0.5,241,ml_signal,143,"Creating a DataFrame for predictions, common in result analysis",,241,,[]
"üß† ML Signal: Evaluating predictions, indicative of model performance assessment","[8582, 100, 254, 10373, 26484, 25, 26439, 11927, 16277, 11, 29105, 286, 2746, 2854, 8922]",0.5,241,ml_signal,143,"Evaluating predictions, indicative of model performance assessment",,241,,[]
"üß† ML Signal: Aggregating metrics, common in model evaluation","[8582, 100, 254, 10373, 26484, 25, 19015, 2301, 803, 20731, 11, 2219, 287, 2746, 12660]",1.0,241,ml_signal,150,"Aggregating metrics, common in model evaluation",,241,,[]
‚úÖ Best Practice: Using pd.concat for combining DataFrames,"[26486, 227, 6705, 19939, 25, 8554, 279, 67, 13, 1102, 9246, 329, 19771, 6060, 35439]",1.0,259,best_practice,159,Using pd.concat for combining DataFrames,,241,            with torch.no_grad():,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 351, 28034, 13, 3919, 62, 9744, 33529]"
‚úÖ Best Practice: Sorting index for organized DataFrame,"[26486, 227, 6705, 19939, 25, 311, 24707, 6376, 329, 8389, 6060, 19778]",0.5,259,best_practice,164,Sorting index for organized DataFrame,,259,,[]
‚ö†Ô∏è SAST Risk (Low): Using mutable default arguments like dict() can lead to unexpected behavior.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 8554, 4517, 540, 4277, 7159, 588, 8633, 3419, 460, 1085, 284, 10059, 4069, 13]",1.0,287,sast_risk,156,Using mutable default arguments like dict() can lead to unexpected behavior.,Low,259,"            feature = data[:, :, : -self.tra.num_states]","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 3895, 796, 1366, 58, 45299, 1058, 11, 1058, 532, 944, 13, 9535, 13, 22510, 62, 27219, 60]"
‚úÖ Best Practice: Use of collections.deque for fixed-size queue is efficient for managing recent items.,"[26486, 227, 6705, 19939, 25, 5765, 286, 17268, 13, 2934, 4188, 329, 5969, 12, 7857, 16834, 318, 6942, 329, 11149, 2274, 3709, 13]",1.0,287,best_practice,164,Use of collections.deque for fixed-size queue is efficient for managing recent items.,,287,,[]
‚úÖ Best Practice: Deep copying state_dict ensures that the original model parameters are not altered.,"[26486, 227, 6705, 19939, 25, 10766, 23345, 1181, 62, 11600, 19047, 326, 262, 2656, 2746, 10007, 389, 407, 14294, 13]",0.5,287,best_practice,183,Deep copying state_dict ensures that the original model parameters are not altered.,,287,,[]
‚ö†Ô∏è SAST Risk (Low): Potential risk of path traversal if logdir is not properly sanitized.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 2526, 286, 3108, 33038, 282, 611, 2604, 15908, 318, 407, 6105, 5336, 36951, 13]",1.0,299,sast_risk,222,Potential risk of path traversal if logdir is not properly sanitized.,Low,287,        self.fitted = True,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 38631, 796, 6407]"
‚ö†Ô∏è SAST Risk (Low): Potential risk of path traversal if logdir is not properly sanitized.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 2526, 286, 3108, 33038, 282, 611, 2604, 15908, 318, 407, 6105, 5336, 36951, 13]",1.0,299,sast_risk,224,Potential risk of path traversal if logdir is not properly sanitized.,Low,299,,[]
‚ö†Ô∏è SAST Risk (Low): Potential risk of path traversal if logdir is not properly sanitized.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 2526, 286, 3108, 33038, 282, 611, 2604, 15908, 318, 407, 6105, 5336, 36951, 13]",1.0,299,sast_risk,224,Potential risk of path traversal if logdir is not properly sanitized.,Low,299,,[]
‚ö†Ô∏è SAST Risk (Low): Potential risk of path traversal if logdir is not properly sanitized.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 2526, 286, 3108, 33038, 282, 611, 2604, 15908, 318, 407, 6105, 5336, 36951, 13]",1.0,335,sast_risk,247,Potential risk of path traversal if logdir is not properly sanitized.,Low,299,"                self.logger.info(""\ttrain metrics: %s"" % train_metrics)","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 59, 926, 3201, 20731, 25, 4064, 82, 1, 4064, 4512, 62, 4164, 10466, 8]"
"üß† ML Signal: Method signature indicates a prediction function, common in ML models","[8582, 100, 254, 10373, 26484, 25, 11789, 9877, 9217, 257, 17724, 2163, 11, 2219, 287, 10373, 4981]",1.0,366,ml_signal,244,"Method signature indicates a prediction function, common in ML models",,335,                train_set.clear_memory()  # NOTE: clear the shared memory,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 4512, 62, 2617, 13, 20063, 62, 31673, 3419, 220, 1303, 24550, 25, 1598, 262, 4888, 4088]"
"‚ö†Ô∏è SAST Risk (Low): Raises an exception if the model is not fitted, which could be a denial of service vector if not handled properly","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 7567, 2696, 281, 6631, 611, 262, 2746, 318, 407, 18235, 11, 543, 714, 307, 257, 14425, 286, 2139, 15879, 611, 407, 12118, 6105]",0.5,396,sast_risk,246,"Raises an exception if the model is not fitted, which could be a denial of service vector if not handled properly",Low,366,"                evals_result[""train""].append(train_metrics)","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 819, 874, 62, 20274, 14692, 27432, 1, 4083, 33295, 7, 27432, 62, 4164, 10466, 8]"
üß† ML Signal: Usage of dataset and segment suggests a pattern for handling data in ML workflows,"[8582, 100, 254, 10373, 26484, 25, 29566, 286, 27039, 290, 10618, 5644, 257, 3912, 329, 9041, 1366, 287, 10373, 670, 44041]",1.0,396,ml_signal,248,Usage of dataset and segment suggests a pattern for handling data in ML workflows,,396,,[]
üß† ML Signal: Method call to test_epoch with return_pred=True indicates a pattern for evaluating models and obtaining predictions,"[8582, 100, 254, 10373, 26484, 25, 11789, 869, 284, 1332, 62, 538, 5374, 351, 1441, 62, 28764, 28, 17821, 9217, 257, 3912, 329, 22232, 4981, 290, 16727, 16277]",0.5,422,ml_signal,250,Method call to test_epoch with return_pred=True indicates a pattern for evaluating models and obtaining predictions,,396,"            evals_result[""valid""].append(valid_metrics)","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 819, 874, 62, 20274, 14692, 12102, 1, 4083, 33295, 7, 12102, 62, 4164, 10466, 8]"
‚úÖ Best Practice: Logging metrics is a good practice for monitoring model performance,"[26486, 227, 6705, 19939, 25, 5972, 2667, 20731, 318, 257, 922, 3357, 329, 9904, 2746, 2854]",0.5,454,best_practice,251,Logging metrics is a good practice for monitoring model performance,,422,"            self.logger.info(""\tvalid metrics: %s"" % valid_metrics)","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 59, 83, 12102, 20731, 25, 4064, 82, 1, 4064, 4938, 62, 4164, 10466, 8]"
"üß† ML Signal: Definition of a class for an LSTM model, indicating a pattern for creating neural network models","[8582, 100, 254, 10373, 26484, 25, 30396, 286, 257, 1398, 329, 281, 406, 2257, 44, 2746, 11, 12739, 257, 3912, 329, 4441, 17019, 3127, 4981]",0.5,480,ml_signal,250,"Definition of a class for an LSTM model, indicating a pattern for creating neural network models",,454,"            evals_result[""valid""].append(valid_metrics)","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 819, 874, 62, 20274, 14692, 12102, 1, 4083, 33295, 7, 12102, 62, 4164, 10466, 8]"
‚úÖ Best Practice: Docstring provides a clear explanation of the class and its parameters,"[26486, 227, 6705, 19939, 25, 14432, 8841, 3769, 257, 1598, 7468, 286, 262, 1398, 290, 663, 10007]",1.0,512,best_practice,251,Docstring provides a clear explanation of the class and its parameters,,480,"            self.logger.info(""\tvalid metrics: %s"" % valid_metrics)","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 59, 83, 12102, 20731, 25, 4064, 82, 1, 4064, 4938, 62, 4164, 10466, 8]"
‚úÖ Best Practice: Call to super() ensures proper initialization of the base class,"[26486, 227, 6705, 19939, 25, 4889, 284, 2208, 3419, 19047, 1774, 37588, 286, 262, 2779, 1398]",0.5,543,best_practice,274,Call to super() ensures proper initialization of the base class,,512,"            self.tra.load_state_dict(params_list[""tra""][-1])","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 9535, 13, 2220, 62, 5219, 62, 11600, 7, 37266, 62, 4868, 14692, 9535, 1, 7131, 12, 16, 12962]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,579,ml_signal,276,Storing model hyperparameters as instance variables,,543,"        self.logger.info(""best score: %.6lf @ %d"" % (best_score, best_epoch))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 13466, 4776, 25, 4064, 13, 21, 1652, 2488, 4064, 67, 1, 4064, 357, 13466, 62, 26675, 11, 1266, 62, 538, 5374, 4008]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,603,ml_signal,278,Storing model hyperparameters as instance variables,,579,"        self.tra.load_state_dict(best_params[""tra""])","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 9535, 13, 2220, 62, 5219, 62, 11600, 7, 13466, 62, 37266, 14692, 9535, 8973, 8]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,632,ml_signal,280,Storing model hyperparameters as instance variables,,603,"        metrics, preds = self.test_epoch(test_set, return_pred=True)","[220, 220, 220, 220, 220, 220, 220, 20731, 11, 2747, 82, 796, 2116, 13, 9288, 62, 538, 5374, 7, 9288, 62, 2617, 11, 1441, 62, 28764, 28, 17821, 8]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,655,ml_signal,281,Storing model hyperparameters as instance variables,,632,"        self.logger.info(""test metrics: %s"" % metrics)","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 9288, 20731, 25, 4064, 82, 1, 4064, 20731, 8]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,678,ml_signal,281,Storing model hyperparameters as instance variables,,655,"        self.logger.info(""test metrics: %s"" % metrics)","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 9288, 20731, 25, 4064, 82, 1, 4064, 20731, 8]"
‚úÖ Best Practice: Using nn.Dropout for regularization,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 26932, 448, 329, 3218, 1634]",1.0,690,best_practice,288,Using nn.Dropout for regularization,,678,            ),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1267]"
‚úÖ Best Practice: Using nn.LSTM for sequence modeling,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 43, 2257, 44, 329, 8379, 21128]",1.0,702,best_practice,288,Using nn.LSTM for sequence modeling,,690,            ),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1267]"
üß† ML Signal: Conditional logic based on use_attn flag,"[8582, 100, 254, 10373, 26484, 25, 9724, 1859, 9156, 1912, 319, 779, 62, 1078, 77, 6056]",0.5,732,ml_signal,296,Conditional logic based on use_attn flag,,702,"                    ""model_config"": self.model_config,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 19849, 62, 11250, 1298, 2116, 13, 19849, 62, 11250, 11]"
‚úÖ Best Practice: Using nn.Linear for attention mechanism,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 14993, 451, 329, 3241, 9030]",1.0,758,best_practice,298,Using nn.Linear for attention mechanism,,732,"                    ""lr"": self.lr,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 14050, 1298, 2116, 13, 14050, 11]"
‚úÖ Best Practice: Using nn.Linear for attention mechanism,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 14993, 451, 329, 3241, 9030]",1.0,788,best_practice,300,Using nn.Linear for attention mechanism,,758,"                    ""early_stop"": self.early_stop,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 11458, 62, 11338, 1298, 2116, 13, 11458, 62, 11338, 11]"
üß† ML Signal: Adjusting output size based on attention usage,"[8582, 100, 254, 10373, 26484, 25, 20292, 278, 5072, 2546, 1912, 319, 3241, 8748]",1.0,828,ml_signal,302,Adjusting output size based on attention usage,,788,"                    ""max_steps_per_epoch"": self.max_steps_per_epoch,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 9806, 62, 20214, 62, 525, 62, 538, 5374, 1298, 2116, 13, 9806, 62, 20214, 62, 525, 62, 538, 5374, 11]"
üß† ML Signal: Adjusting output size based on attention usage,"[8582, 100, 254, 10373, 26484, 25, 20292, 278, 5072, 2546, 1912, 319, 3241, 8748]",1.0,854,ml_signal,305,Adjusting output size based on attention usage,,828,"                    ""seed"": self.seed,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 28826, 1298, 2116, 13, 28826, 11]"
üß† ML Signal: Use of dropout indicates a regularization technique,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 4268, 448, 9217, 257, 3218, 1634, 8173]",0.5,873,ml_signal,295,Use of dropout indicates a regularization technique,,854,"                ""config"": {","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 11250, 1298, 1391]"
üß† ML Signal: Conditional logic based on training mode,"[8582, 100, 254, 10373, 26484, 25, 9724, 1859, 9156, 1912, 319, 3047, 4235]",1.0,903,ml_signal,297,Conditional logic based on training mode,,873,"                    ""tra_config"": self.tra_config,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 9535, 62, 11250, 1298, 2116, 13, 9535, 62, 11250, 11]"
üß† ML Signal: Adding noise to input as a form of data augmentation,"[8582, 100, 254, 10373, 26484, 25, 18247, 7838, 284, 5128, 355, 257, 1296, 286, 1366, 16339, 14374]",0.5,929,ml_signal,298,Adding noise to input as a form of data augmentation,,903,"                    ""lr"": self.lr,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 14050, 1298, 2116, 13, 14050, 11]"
‚ö†Ô∏è SAST Risk (Low): Potential for device mismatch if `x` is not on the same device as `noise`,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 329, 3335, 46318, 611, 4600, 87, 63, 318, 407, 319, 262, 976, 3335, 355, 4600, 3919, 786, 63]",0.5,959,sast_risk,300,Potential for device mismatch if `x` is not on the same device as `noise`,Low,929,"                    ""early_stop"": self.early_stop,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 11458, 62, 11338, 1298, 2116, 13, 11458, 62, 11338, 11]"
üß† ML Signal: Use of RNN layer for sequence processing,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 371, 6144, 7679, 329, 8379, 7587]",0.5,987,ml_signal,303,Use of RNN layer for sequence processing,,959,"                    ""lamb"": self.lamb,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 2543, 65, 1298, 2116, 13, 2543, 65, 11]"
üß† ML Signal: Extracting the last output from RNN for further processing,"[8582, 100, 254, 10373, 26484, 25, 29677, 278, 262, 938, 5072, 422, 371, 6144, 329, 2252, 7587]",1.0,1013,ml_signal,305,Extracting the last output from RNN for further processing,,987,"                    ""seed"": self.seed,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 28826, 1298, 2116, 13, 28826, 11]"
üß† ML Signal: Use of attention mechanism,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 3241, 9030]",1.0,1029,ml_signal,307,Use of attention mechanism,,1013,"                },","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 8964]"
üß† ML Signal: Linear transformation followed by non-linearity,"[8582, 100, 254, 10373, 26484, 25, 44800, 13389, 3940, 416, 1729, 12, 29127, 414]",1.0,1050,ml_signal,309,Linear transformation followed by non-linearity,,1029,"                ""metric"": metrics,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 4164, 1173, 1298, 20731, 11]"
üß† ML Signal: Use of softmax for attention score calculation,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 2705, 9806, 329, 3241, 4776, 17952]",0.5,1080,ml_signal,311,Use of softmax for attention score calculation,,1050,"            with open(self.logdir + ""/info.json"", ""w"") as f:","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 351, 1280, 7, 944, 13, 6404, 15908, 1343, 12813, 10951, 13, 17752, 1600, 366, 86, 4943, 355, 277, 25]"
üß† ML Signal: Weighted sum for attention output,"[8582, 100, 254, 10373, 26484, 25, 14331, 276, 2160, 329, 3241, 5072]",0.5,1080,ml_signal,313,Weighted sum for attention output,,1080,,[]
üß† ML Signal: Concatenating attention output with last RNN output,"[8582, 100, 254, 10373, 26484, 25, 1482, 9246, 268, 803, 3241, 5072, 351, 938, 371, 6144, 5072]",0.5,1093,ml_signal,315,Concatenating attention output with last RNN output,,1080,        if not self.fitted:,"[220, 220, 220, 220, 220, 220, 220, 611, 407, 2116, 13, 38631, 25]"
üß† ML Signal: Custom neural network module definition,"[8582, 100, 254, 10373, 26484, 25, 8562, 17019, 3127, 8265, 6770]",1.0,1109,ml_signal,307,Custom neural network module definition,,1093,"                },","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 8964]"
‚úÖ Best Practice: Call to super() ensures proper initialization of the base class,"[26486, 227, 6705, 19939, 25, 4889, 284, 2208, 3419, 19047, 1774, 37588, 286, 262, 2779, 1398]",1.0,1130,best_practice,309,Call to super() ensures proper initialization of the base class,,1109,"                ""metric"": metrics,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 366, 4164, 1173, 1298, 20731, 11]"
"üß† ML Signal: Usage of dropout, common in training neural networks to prevent overfitting","[8582, 100, 254, 10373, 26484, 25, 29566, 286, 4268, 448, 11, 2219, 287, 3047, 17019, 7686, 284, 2948, 625, 32232]",1.0,1160,ml_signal,311,"Usage of dropout, common in training neural networks to prevent overfitting",,1130,"            with open(self.logdir + ""/info.json"", ""w"") as f:","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 351, 1280, 7, 944, 13, 6404, 15908, 1343, 12813, 10951, 13, 17752, 1600, 366, 86, 4943, 355, 277, 25]"
"üß† ML Signal: Initialization of positional encoding matrix, a common pattern in transformer models","[8582, 100, 254, 10373, 26484, 25, 20768, 1634, 286, 45203, 21004, 17593, 11, 257, 2219, 3912, 287, 47385, 4981]",1.0,1160,ml_signal,313,"Initialization of positional encoding matrix, a common pattern in transformer models",,1160,,[]
üß† ML Signal: Use of torch.arange to create a sequence of positions,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 28034, 13, 283, 858, 284, 2251, 257, 8379, 286, 6116]",1.0,1173,ml_signal,315,Use of torch.arange to create a sequence of positions,,1160,        if not self.fitted:,"[220, 220, 220, 220, 220, 220, 220, 611, 407, 2116, 13, 38631, 25]"
"üß† ML Signal: Calculation of div_term for scaling positions, typical in positional encoding","[8582, 100, 254, 10373, 26484, 25, 2199, 14902, 286, 2659, 62, 4354, 329, 20796, 6116, 11, 7226, 287, 45203, 21004]",0.5,1173,ml_signal,317,"Calculation of div_term for scaling positions, typical in positional encoding",,1173,,[]
üß† ML Signal: Application of sine function to even indices for positional encoding,"[8582, 100, 254, 10373, 26484, 25, 15678, 286, 264, 500, 2163, 284, 772, 36525, 329, 45203, 21004]",0.5,1173,ml_signal,319,Application of sine function to even indices for positional encoding,,1173,,[]
üß† ML Signal: Application of cosine function to odd indices for positional encoding,"[8582, 100, 254, 10373, 26484, 25, 15678, 286, 8615, 500, 2163, 284, 5629, 36525, 329, 45203, 21004]",0.5,1196,ml_signal,321,Application of cosine function to odd indices for positional encoding,,1173,"        self.logger.info(""test metrics: %s"" % metrics)","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 9288, 20731, 25, 4064, 82, 1, 4064, 20731, 8]"
üß† ML Signal: Reshaping positional encoding for batch processing,"[8582, 100, 254, 10373, 26484, 25, 1874, 71, 9269, 45203, 21004, 329, 15458, 7587]",1.0,1196,ml_signal,322,Reshaping positional encoding for batch processing,,1196,,[]
‚úÖ Best Practice: Use of register_buffer to store non-parameter tensors,"[26486, 227, 6705, 19939, 25, 5765, 286, 7881, 62, 22252, 284, 3650, 1729, 12, 17143, 2357, 11192, 669]",1.0,1196,best_practice,322,Use of register_buffer to store non-parameter tensors,,1196,,[]
"üß† ML Signal: Method definition in a class, common in ML models for forward pass","[8582, 100, 254, 10373, 26484, 25, 11789, 6770, 287, 257, 1398, 11, 2219, 287, 10373, 4981, 329, 2651, 1208]",0.5,1215,ml_signal,318,"Method definition in a class, common in ML models for forward pass",,1196,        test_set = dataset.prepare(segment),"[220, 220, 220, 220, 220, 220, 220, 1332, 62, 2617, 796, 27039, 13, 46012, 533, 7, 325, 5154, 8]"
"üß† ML Signal: Usage of positional encoding, common in transformer models","[8582, 100, 254, 10373, 26484, 25, 29566, 286, 45203, 21004, 11, 2219, 287, 47385, 4981]",0.5,1244,ml_signal,320,"Usage of positional encoding, common in transformer models",,1215,"        metrics, preds = self.test_epoch(test_set, return_pred=True)","[220, 220, 220, 220, 220, 220, 220, 20731, 11, 2747, 82, 796, 2116, 13, 9288, 62, 538, 5374, 7, 9288, 62, 2617, 11, 1441, 62, 28764, 28, 17821, 8]"
"üß† ML Signal: Use of dropout, a common technique for regularization in neural networks","[8582, 100, 254, 10373, 26484, 25, 5765, 286, 4268, 448, 11, 257, 2219, 8173, 329, 3218, 1634, 287, 17019, 7686]",1.0,1244,ml_signal,322,"Use of dropout, a common technique for regularization in neural networks",,1244,,[]
"üß† ML Signal: Definition of a Transformer model class, useful for identifying model architecture patterns","[8582, 100, 254, 10373, 26484, 25, 30396, 286, 257, 3602, 16354, 2746, 1398, 11, 4465, 329, 13720, 2746, 10959, 7572]",1.0,1267,ml_signal,321,"Definition of a Transformer model class, useful for identifying model architecture patterns",,1244,"        self.logger.info(""test metrics: %s"" % metrics)","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 9288, 20731, 25, 4064, 82, 1, 4064, 20731, 8]"
‚úÖ Best Practice: Docstring provides a clear explanation of the class and its parameters,"[26486, 227, 6705, 19939, 25, 14432, 8841, 3769, 257, 1598, 7468, 286, 262, 1398, 290, 663, 10007]",0.5,1267,best_practice,322,Docstring provides a clear explanation of the class and its parameters,,1267,,[]
‚úÖ Best Practice: Call to super() ensures proper initialization of the base class,"[26486, 227, 6705, 19939, 25, 4889, 284, 2208, 3419, 19047, 1774, 37588, 286, 262, 2779, 1398]",1.0,1280,best_practice,343,Call to super() ensures proper initialization of the base class,,1267,"        hidden_size=64,","[220, 220, 220, 220, 220, 220, 220, 7104, 62, 7857, 28, 2414, 11]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,1294,ml_signal,345,Storing model hyperparameters as instance variables,,1280,"        use_attn=True,","[220, 220, 220, 220, 220, 220, 220, 779, 62, 1078, 77, 28, 17821, 11]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,1309,ml_signal,347,Storing model hyperparameters as instance variables,,1294,"        input_drop=0.0,","[220, 220, 220, 220, 220, 220, 220, 5128, 62, 14781, 28, 15, 13, 15, 11]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,1319,ml_signal,349,Storing model hyperparameters as instance variables,,1309,"        *args,","[220, 220, 220, 220, 220, 220, 220, 1635, 22046, 11]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,1323,ml_signal,351,Storing model hyperparameters as instance variables,,1319,    ):,"[220, 220, 220, 15179]"
üß† ML Signal: Storing model hyperparameters as instance variables,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 8718, 17143, 7307, 355, 4554, 9633]",1.0,1336,ml_signal,352,Storing model hyperparameters as instance variables,,1323,        super().__init__(),"[220, 220, 220, 220, 220, 220, 220, 2208, 22446, 834, 15003, 834, 3419]"
‚úÖ Best Practice: Using nn.Dropout for regularization,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 26932, 448, 329, 3218, 1634]",1.0,1352,best_practice,355,Using nn.Dropout for regularization,,1336,        self.hidden_size = hidden_size,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 30342, 62, 7857, 796, 7104, 62, 7857]"
‚úÖ Best Practice: Using nn.Linear for input projection,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 14993, 451, 329, 5128, 20128]",1.0,1370,best_practice,357,Using nn.Linear for input projection,,1352,        self.use_attn = use_attn,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 1904, 62, 1078, 77, 796, 779, 62, 1078, 77]"
‚úÖ Best Practice: Using a separate class for positional encoding,"[26486, 227, 6705, 19939, 25, 8554, 257, 4553, 1398, 329, 45203, 21004]",1.0,1370,best_practice,359,Using a separate class for positional encoding,,1370,,[]
‚úÖ Best Practice: Using nn.TransformerEncoderLayer for modularity,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 8291, 16354, 27195, 12342, 49925, 329, 26507, 414]",1.0,1370,best_practice,361,Using nn.TransformerEncoderLayer for modularity,,1370,,[]
‚úÖ Best Practice: Using nn.TransformerEncoder for sequence modeling,"[26486, 227, 6705, 19939, 25, 8554, 299, 77, 13, 8291, 16354, 27195, 12342, 329, 8379, 21128]",1.0,1391,best_practice,365,Using nn.TransformerEncoder for sequence modeling,,1370,"            num_layers=num_layers,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 997, 62, 75, 6962, 28, 22510, 62, 75, 6962, 11]"
üß† ML Signal: Storing model output size as an instance variable,"[8582, 100, 254, 10373, 26484, 25, 520, 3255, 2746, 5072, 2546, 355, 281, 4554, 7885]",1.0,1408,ml_signal,367,Storing model output size as an instance variable,,1391,"            dropout=dropout,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 4268, 448, 28, 14781, 448, 11]"
üß† ML Signal: Use of dropout indicates a training mode pattern,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 4268, 448, 9217, 257, 3047, 4235, 3912]",1.0,1425,ml_signal,358,Use of dropout indicates a training mode pattern,,1408,        self.noise_level = noise_level,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 3919, 786, 62, 5715, 796, 7838, 62, 5715]"
üß† ML Signal: Conditional logic based on training mode,"[8582, 100, 254, 10373, 26484, 25, 9724, 1859, 9156, 1912, 319, 3047, 4235]",0.5,1448,ml_signal,360,Conditional logic based on training mode,,1425,        self.input_drop = nn.Dropout(input_drop),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 15414, 62, 14781, 796, 299, 77, 13, 26932, 448, 7, 15414, 62, 14781, 8]"
üß† ML Signal: Adding noise to input data is a common data augmentation technique,"[8582, 100, 254, 10373, 26484, 25, 18247, 7838, 284, 5128, 1366, 318, 257, 2219, 1366, 16339, 14374, 8173]",1.0,1448,ml_signal,361,Adding noise to input data is a common data augmentation technique,,1448,,[]
‚ö†Ô∏è SAST Risk (Low): Potential for device mismatch if `x` is not on the same device as `noise`,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 329, 3335, 46318, 611, 4600, 87, 63, 318, 407, 319, 262, 976, 3335, 355, 4600, 3919, 786, 63]",1.0,1467,sast_risk,363,Potential for device mismatch if `x` is not on the same device as `noise`,Low,1448,"            input_size=input_size,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5128, 62, 7857, 28, 15414, 62, 7857, 11]"
‚úÖ Best Practice: Use of `contiguous` to ensure memory layout is suitable for further operations,"[26486, 227, 6705, 19939, 25, 5765, 286, 4600, 3642, 29709, 63, 284, 4155, 4088, 12461, 318, 11080, 329, 2252, 4560]",0.5,1484,best_practice,366,Use of `contiguous` to ensure memory layout is suitable for further operations,,1467,"            batch_first=True,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 15458, 62, 11085, 28, 17821, 11]"
üß† ML Signal: Use of positional encoding in sequence models,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 45203, 21004, 287, 8379, 4981]",0.5,1492,ml_signal,368,Use of positional encoding in sequence models,,1484,        ),"[220, 220, 220, 220, 220, 220, 220, 1267]"
üß† ML Signal: Projection layer applied to input data,"[8582, 100, 254, 10373, 26484, 25, 4935, 295, 7679, 5625, 284, 5128, 1366]",0.5,1500,ml_signal,368,Projection layer applied to input data,,1492,        ),"[220, 220, 220, 220, 220, 220, 220, 1267]"
üß† ML Signal: Use of encoder suggests a transformer or similar architecture,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 2207, 12342, 5644, 257, 47385, 393, 2092, 10959]",0.5,1508,ml_signal,368,Use of encoder suggests a transformer or similar architecture,,1500,        ),"[220, 220, 220, 220, 220, 220, 220, 1267]"
‚ö†Ô∏è SAST Risk (Low): Accessing the last element of `out` assumes it is non-empty,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 8798, 278, 262, 938, 5002, 286, 4600, 448, 63, 18533, 340, 318, 1729, 12, 28920]",0.5,1525,sast_risk,378,Accessing the last element of `out` assumes it is non-empty,Low,1508,        x = self.input_drop(x),"[220, 220, 220, 220, 220, 220, 220, 2124, 796, 2116, 13, 15414, 62, 14781, 7, 87, 8]"
"üß† ML Signal: Class definition for a neural network module, indicating a pattern for model architecture","[8582, 100, 254, 10373, 26484, 25, 5016, 6770, 329, 257, 17019, 3127, 8265, 11, 12739, 257, 3912, 329, 2746, 10959]",0.5,1542,ml_signal,367,"Class definition for a neural network module, indicating a pattern for model architecture",,1525,"            dropout=dropout,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 4268, 448, 28, 14781, 448, 11]"
‚úÖ Best Practice: Use of default values for function parameters improves usability and flexibility.,"[26486, 227, 6705, 19939, 25, 5765, 286, 4277, 3815, 329, 2163, 10007, 19575, 42863, 290, 13688, 13]",1.0,1542,best_practice,379,Use of default values for function parameters improves usability and flexibility.,,1542,,[]
‚úÖ Best Practice: Conditional initialization of components based on parameters enhances efficiency.,"[26486, 227, 6705, 19939, 25, 9724, 1859, 37588, 286, 6805, 1912, 319, 10007, 32479, 9332, 13]",1.0,1563,best_practice,384,Conditional initialization of components based on parameters enhances efficiency.,,1542,"        rnn_out, _ = self.rnn(x)","[220, 220, 220, 220, 220, 220, 220, 374, 20471, 62, 448, 11, 4808, 796, 2116, 13, 81, 20471, 7, 87, 8]"
"üß† ML Signal: Use of nn.Linear indicates a neural network model, which is common in ML applications.","[8582, 100, 254, 10373, 26484, 25, 5765, 286, 299, 77, 13, 14993, 451, 9217, 257, 17019, 3127, 2746, 11, 543, 318, 2219, 287, 10373, 5479, 13]",0.5,1574,ml_signal,393,"Use of nn.Linear indicates a neural network model, which is common in ML applications.",,1563,        return last_out,"[220, 220, 220, 220, 220, 220, 220, 1441, 938, 62, 448]"
‚ö†Ô∏è SAST Risk (Low): Using random values can lead to non-deterministic behavior,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 8554, 4738, 3815, 460, 1085, 284, 1729, 12, 67, 2357, 49228, 4069]",1.0,1597,sast_risk,400,Using random values can lead to non-deterministic behavior,Low,1574,        self.dropout = nn.Dropout(p=dropout),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 14781, 448, 796, 299, 77, 13, 26932, 448, 7, 79, 28, 14781, 448, 8]"
‚ö†Ô∏è SAST Risk (Low): Using random values can lead to non-deterministic behavior,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 8554, 4738, 3815, 460, 1085, 284, 1729, 12, 67, 2357, 49228, 4069]",1.0,1622,sast_risk,405,Using random values can lead to non-deterministic behavior,Low,1597,"        pe[:, 0::2] = torch.sin(position * div_term)","[220, 220, 220, 220, 220, 220, 220, 613, 58, 45299, 657, 3712, 17, 60, 796, 28034, 13, 31369, 7, 9150, 1635, 2659, 62, 4354, 8]"
üß† ML Signal: Use of gumbel_softmax indicates probabilistic decision-making,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 308, 2178, 417, 62, 4215, 9806, 9217, 1861, 14991, 2569, 2551, 12, 8601]",1.0,1639,ml_signal,408,Use of gumbel_softmax indicates probabilistic decision-making,,1622,"        self.register_buffer(""pe"", pe)","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 30238, 62, 22252, 7203, 431, 1600, 613, 8]"
üß† ML Signal: Different behavior during training and inference,"[8582, 100, 254, 10373, 26484, 25, 20615, 4069, 1141, 3047, 290, 32278]",1.0,1663,ml_signal,411,Different behavior during training and inference,,1639,"        x = x + self.pe[: x.size(0), :]","[220, 220, 220, 220, 220, 220, 220, 2124, 796, 2124, 1343, 2116, 13, 431, 58, 25, 2124, 13, 7857, 7, 15, 828, 1058, 60]"
üß† ML Signal: Different behavior during training and inference,"[8582, 100, 254, 10373, 26484, 25, 20615, 4069, 1141, 3047, 290, 32278]",1.0,1663,ml_signal,414,Different behavior during training and inference,,1663,,[]
üß† ML Signal: Function for evaluating prediction accuracy,"[8582, 100, 254, 10373, 26484, 25, 15553, 329, 22232, 17724, 9922]",0.5,1678,ml_signal,412,Function for evaluating prediction accuracy,,1663,        return self.dropout(x),"[220, 220, 220, 220, 220, 220, 220, 1441, 2116, 13, 14781, 448, 7, 87, 8]"
üß† ML Signal: Usage of rank and percentage transformation,"[8582, 100, 254, 10373, 26484, 25, 29566, 286, 4279, 290, 5873, 13389]",0.5,1678,ml_signal,414,Usage of rank and percentage transformation,,1678,,[]
‚ö†Ô∏è SAST Risk (Low): Assumes pred has 'score' and 'label' attributes,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2195, 8139, 2747, 468, 705, 26675, 6, 290, 705, 18242, 6, 12608]",1.0,1685,sast_risk,416,Assumes pred has 'score' and 'label' attributes,Low,1678,"    """"""Transformer Model","[220, 220, 220, 37227, 8291, 16354, 9104]"
‚ö†Ô∏è SAST Risk (Low): Assumes pred has 'label' attribute,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2195, 8139, 2747, 468, 705, 18242, 6, 11688]",0.5,1691,sast_risk,418,Assumes pred has 'label' attribute,Low,1685,    Args:,"[220, 220, 220, 943, 14542, 25]"
üß† ML Signal: Calculation of difference between score and label,"[8582, 100, 254, 10373, 26484, 25, 2199, 14902, 286, 3580, 1022, 4776, 290, 6167]",1.0,1706,ml_signal,420,Calculation of difference between score and label,,1691,        hidden_size (int): hidden size,"[220, 220, 220, 220, 220, 220, 220, 7104, 62, 7857, 357, 600, 2599, 7104, 2546]"
üß† ML Signal: Calculation of Mean Squared Error,"[8582, 100, 254, 10373, 26484, 25, 2199, 14902, 286, 22728, 5056, 1144, 13047]",0.5,1724,ml_signal,422,Calculation of Mean Squared Error,,1706,        num_heads (int): number of heads in transformer,"[220, 220, 220, 220, 220, 220, 220, 997, 62, 16600, 357, 600, 2599, 1271, 286, 6665, 287, 47385]"
üß† ML Signal: Calculation of Mean Absolute Error,"[8582, 100, 254, 10373, 26484, 25, 2199, 14902, 286, 22728, 36532, 13047]",0.5,1744,ml_signal,424,Calculation of Mean Absolute Error,,1724,        input_drop (float): input dropout for data augmentation,"[220, 220, 220, 220, 220, 220, 220, 5128, 62, 14781, 357, 22468, 2599, 5128, 4268, 448, 329, 1366, 16339, 14374]"
üß† ML Signal: Calculation of correlation between score and label,"[8582, 100, 254, 10373, 26484, 25, 2199, 14902, 286, 16096, 1022, 4776, 290, 6167]",0.5,1748,ml_signal,426,Calculation of correlation between score and label,,1744,"    """"""","[220, 220, 220, 37227]"
‚úÖ Best Practice: Return a dictionary for structured results,"[26486, 227, 6705, 19939, 25, 8229, 257, 22155, 329, 20793, 2482]",1.0,1756,best_practice,428,Return a dictionary for structured results,,1748,    def __init__(,"[220, 220, 220, 825, 11593, 15003, 834, 7]"
‚ö†Ô∏è SAST Risk (Low): Use of assert statement for type checking can be bypassed if Python is run with optimizations.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 5765, 286, 6818, 2643, 329, 2099, 10627, 460, 307, 17286, 276, 611, 11361, 318, 1057, 351, 41446, 13]",1.0,1774,sast_risk,421,Use of assert statement for type checking can be bypassed if Python is run with optimizations.,Low,1756,        num_layers (int): number of transformer layers,"[220, 220, 220, 220, 220, 220, 220, 997, 62, 75, 6962, 357, 600, 2599, 1271, 286, 47385, 11685]"
‚úÖ Best Practice: Consider using isinstance() for type checking outside of assert for better error handling.,"[26486, 227, 6705, 19939, 25, 12642, 1262, 318, 39098, 3419, 329, 2099, 10627, 2354, 286, 6818, 329, 1365, 4049, 9041, 13]",0.5,1792,best_practice,422,Consider using isinstance() for type checking outside of assert for better error handling.,,1774,        num_heads (int): number of heads in transformer,"[220, 220, 220, 220, 220, 220, 220, 997, 62, 16600, 357, 600, 2599, 1271, 286, 6665, 287, 47385]"
‚úÖ Best Practice: Use collections.abc for abstract base classes to ensure compatibility with different collection types.,"[26486, 227, 6705, 19939, 25, 5765, 17268, 13, 39305, 329, 12531, 2779, 6097, 284, 4155, 17764, 351, 1180, 4947, 3858, 13]",0.5,1812,best_practice,424,Use collections.abc for abstract base classes to ensure compatibility with different collection types.,,1792,        input_drop (float): input dropout for data augmentation,"[220, 220, 220, 220, 220, 220, 220, 5128, 62, 14781, 357, 22468, 2599, 5128, 4268, 448, 329, 1366, 16339, 14374]"
‚úÖ Best Practice: Use collections.defaultdict for automatic initialization of dictionary values.,"[26486, 227, 6705, 19939, 25, 5765, 17268, 13, 12286, 11600, 329, 11353, 37588, 286, 22155, 3815, 13]",0.5,1821,best_practice,429,Use collections.defaultdict for automatic initialization of dictionary values.,,1812,"        self,","[220, 220, 220, 220, 220, 220, 220, 2116, 11]"
"‚ö†Ô∏è SAST Risk (Low): Function does not handle cases where inp_tensor is not a tensor, which could lead to runtime errors.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 15553, 857, 407, 5412, 2663, 810, 287, 79, 62, 83, 22854, 318, 407, 257, 11192, 273, 11, 543, 714, 1085, 284, 19124, 8563, 13]",0.5,1834,sast_risk,439,"Function does not handle cases where inp_tensor is not a tensor, which could lead to runtime errors.",Low,1821,        super().__init__(),"[220, 220, 220, 220, 220, 220, 220, 2208, 22446, 834, 15003, 834, 3419]"
"‚ö†Ô∏è SAST Risk (Low): Assumes torch is imported and available in the namespace, which may not always be the case.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2195, 8139, 28034, 318, 17392, 290, 1695, 287, 262, 25745, 11, 543, 743, 407, 1464, 307, 262, 1339, 13]",0.5,1850,sast_risk,442,"Assumes torch is imported and available in the namespace, which may not always be the case.",Low,1834,        self.hidden_size = hidden_size,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 30342, 62, 7857, 796, 7104, 62, 7857]"
"‚ö†Ô∏è SAST Risk (Low): Assumes inp_tensor is a tensor with a valid shape, which may not always be the case.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2195, 8139, 287, 79, 62, 83, 22854, 318, 257, 11192, 273, 351, 257, 4938, 5485, 11, 543, 743, 407, 1464, 307, 262, 1339, 13]",0.5,1866,sast_risk,444,"Assumes inp_tensor is a tensor with a valid shape, which may not always be the case.",Low,1850,        self.num_heads = num_heads,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 22510, 62, 16600, 796, 997, 62, 16600]"
‚úÖ Best Practice: Check the length of ind to handle tensors of different dimensions.,"[26486, 227, 6705, 19939, 25, 6822, 262, 4129, 286, 773, 284, 5412, 11192, 669, 286, 1180, 15225, 13]",0.5,1866,best_practice,448,Check the length of ind to handle tensors of different dimensions.,,1866,,[]
"‚ö†Ô∏è SAST Risk (Low): Assumes inp_tensor has non-inf values to compute max, which may not always be the case.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2195, 8139, 287, 79, 62, 83, 22854, 468, 1729, 12, 10745, 3815, 284, 24061, 3509, 11, 543, 743, 407, 1464, 307, 262, 1339, 13]",0.5,1908,sast_risk,453,"Assumes inp_tensor has non-inf values to compute max, which may not always be the case.",Low,1866,"            nhead=num_heads, dropout=dropout, d_model=hidden_size, dim_feedforward=hidden_size * 4","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 299, 2256, 28, 22510, 62, 16600, 11, 4268, 448, 28, 14781, 448, 11, 288, 62, 19849, 28, 30342, 62, 7857, 11, 5391, 62, 12363, 11813, 28, 30342, 62, 7857, 1635, 604]"
"üß† ML Signal: Function modifies tensor in place, which is a common pattern in tensor manipulation.","[8582, 100, 254, 10373, 26484, 25, 15553, 953, 6945, 11192, 273, 287, 1295, 11, 543, 318, 257, 2219, 3912, 287, 11192, 273, 17512, 13]",0.5,1925,ml_signal,460,"Function modifies tensor in place, which is a common pattern in tensor manipulation.",,1908,        x = self.input_drop(x),"[220, 220, 220, 220, 220, 220, 220, 2124, 796, 2116, 13, 15414, 62, 14781, 7, 87, 8]"
‚ö†Ô∏è SAST Risk (Medium): Use of torch.no_grad() can lead to silent errors if gradients are needed later.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 31205, 2599, 5765, 286, 28034, 13, 3919, 62, 9744, 3419, 460, 1085, 284, 10574, 8563, 611, 3915, 2334, 389, 2622, 1568, 13]",0.5,1925,sast_risk,456,Use of torch.no_grad() can lead to silent errors if gradients are needed later.,Medium,1925,,[]
"‚ö†Ô∏è SAST Risk (Low): Function assumes Q is a tensor, lack of input validation.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 15553, 18533, 1195, 318, 257, 11192, 273, 11, 3092, 286, 5128, 21201, 13]",0.5,1925,sast_risk,458,"Function assumes Q is a tensor, lack of input validation.",Low,1925,,[]
"‚ö†Ô∏è SAST Risk (Low): shoot_infs function is used without context, potential for unexpected behavior.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2686, 62, 259, 9501, 2163, 318, 973, 1231, 4732, 11, 2785, 329, 10059, 4069, 13]",0.5,1942,sast_risk,460,"shoot_infs function is used without context, potential for unexpected behavior.",Low,1925,        x = self.input_drop(x),"[220, 220, 220, 220, 220, 220, 220, 2124, 796, 2116, 13, 15414, 62, 14781, 7, 87, 8]"
‚ö†Ô∏è SAST Risk (Low): Direct use of torch.exp can lead to overflow if Q/epsilon is too large.,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 4128, 779, 286, 28034, 13, 11201, 460, 1085, 284, 30343, 611, 1195, 14, 538, 18217, 261, 318, 1165, 1588, 13]",0.5,1963,sast_risk,462,Direct use of torch.exp can lead to overflow if Q/epsilon is too large.,Low,1942,        if self.training and self.noise_level > 0:,"[220, 220, 220, 220, 220, 220, 220, 611, 2116, 13, 34409, 290, 2116, 13, 3919, 786, 62, 5715, 1875, 657, 25]"
"üß† ML Signal: Iterative normalization pattern, common in ML algorithms.","[8582, 100, 254, 10373, 26484, 25, 40806, 876, 3487, 1634, 3912, 11, 2219, 287, 10373, 16113, 13]",0.5,1989,ml_signal,463,"Iterative normalization pattern, common in ML algorithms.",,1963,            noise = torch.randn_like(x).to(x),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 7838, 796, 28034, 13, 25192, 77, 62, 2339, 7, 87, 737, 1462, 7, 87, 8]"
"üß† ML Signal: Iterative normalization pattern, common in ML algorithms.","[8582, 100, 254, 10373, 26484, 25, 40806, 876, 3487, 1634, 3912, 11, 2219, 287, 10373, 16113, 13]",0.5,2015,ml_signal,463,"Iterative normalization pattern, common in ML algorithms.",,1989,            noise = torch.randn_like(x).to(x),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 7838, 796, 28034, 13, 25192, 77, 62, 2339, 7, 87, 737, 1462, 7, 87, 8]"
