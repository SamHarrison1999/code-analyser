annotation,annotation_tokens,confidence,end_token,label,line,reason,severity,start_token,text,tokens
✅ Best Practice: Use of relative imports for better module structure and maintainability,"[26486, 227, 6705, 19939, 25, 5765, 286, 3585, 17944, 329, 1365, 8265, 4645, 290, 5529, 1799]",1.0,5,best_practice,7,Use of relative imports for better module structure and maintainability,,0,import numpy as np,"[11748, 299, 32152, 355, 45941]"
✅ Best Practice: Use of relative imports for better module structure and maintainability,"[26486, 227, 6705, 19939, 25, 5765, 286, 3585, 17944, 329, 1365, 8265, 4645, 290, 5529, 1799]",1.0,11,best_practice,9,Use of relative imports for better module structure and maintainability,,5,"from typing import Text, Union","[6738, 19720, 1330, 8255, 11, 4479]"
✅ Best Practice: Use of relative imports for better module structure and maintainability,"[26486, 227, 6705, 19939, 25, 5765, 286, 3585, 17944, 329, 1365, 8265, 4645, 290, 5529, 1799]",1.0,18,best_practice,15,Use of relative imports for better module structure and maintainability,,11,import torch.nn as nn,"[11748, 28034, 13, 20471, 355, 299, 77]"
✅ Best Practice: Use of relative imports for better module structure and maintainability,"[26486, 227, 6705, 19939, 25, 5765, 286, 3585, 17944, 329, 1365, 8265, 4645, 290, 5529, 1799]",1.0,24,best_practice,17,Use of relative imports for better module structure and maintainability,,18,import torch.optim as optim,"[11748, 28034, 13, 40085, 355, 6436]"
✅ Best Practice: Use of relative imports for better module structure and maintainability,"[26486, 227, 6705, 19939, 25, 5765, 286, 3585, 17944, 329, 1365, 8265, 4645, 290, 5529, 1799]",1.0,24,best_practice,18,Use of relative imports for better module structure and maintainability,,24,,[]
✅ Best Practice: Use of relative imports for better module structure and maintainability,"[26486, 227, 6705, 19939, 25, 5765, 286, 3585, 17944, 329, 1365, 8265, 4645, 290, 5529, 1799]",1.0,24,best_practice,18,Use of relative imports for better module structure and maintainability,,24,,[]
✅ Best Practice: Class names should follow the CapWords convention for readability,"[26486, 227, 6705, 19939, 25, 5016, 3891, 815, 1061, 262, 4476, 37117, 9831, 329, 1100, 1799]",1.0,30,best_practice,17,Class names should follow the CapWords convention for readability,,24,import torch.optim as optim,"[11748, 28034, 13, 40085, 355, 6436]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,41,ml_signal,34,Use of Xavier and orthogonal initialization for weights,,30,"        device=""cpu"",","[220, 220, 220, 220, 220, 220, 220, 3335, 2625, 36166, 1600]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,54,ml_signal,36,Use of Xavier and orthogonal initialization for weights,,41,        super().__init__(),"[220, 220, 220, 220, 220, 220, 220, 2208, 22446, 834, 15003, 834, 3419]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,70,ml_signal,39,Use of Xavier and orthogonal initialization for weights,,54,        self.output_dim = output_dim,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 22915, 62, 27740, 796, 5072, 62, 27740]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,86,ml_signal,41,Use of Xavier and orthogonal initialization for weights,,70,        self.hidden_dim = hidden_size,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 30342, 62, 27740, 796, 7104, 62, 7857]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,129,ml_signal,44,Use of Xavier and orthogonal initialization for weights,,86,"        self.W_i = nn.Parameter(init.xavier_uniform_(torch.empty((self.input_dim, self.hidden_dim))))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 54, 62, 72, 796, 299, 77, 13, 36301, 7, 15003, 13, 87, 19492, 62, 403, 6933, 41052, 13165, 354, 13, 28920, 19510, 944, 13, 15414, 62, 27740, 11, 2116, 13, 30342, 62, 27740, 35514]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,159,ml_signal,46,Use of Xavier and orthogonal initialization for weights,,129,        self.b_i = nn.Parameter(torch.zeros(self.hidden_dim)),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 65, 62, 72, 796, 299, 77, 13, 36301, 7, 13165, 354, 13, 9107, 418, 7, 944, 13, 30342, 62, 27740, 4008]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,200,ml_signal,49,Use of Xavier and orthogonal initialization for weights,,159,"        self.U_ste = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 52, 62, 4169, 796, 299, 77, 13, 36301, 7, 15003, 13, 1506, 519, 20996, 41052, 13165, 354, 13, 28920, 7, 944, 13, 30342, 62, 27740, 11, 2116, 13, 30342, 62, 27740, 22305]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,200,ml_signal,51,Use of Xavier and orthogonal initialization for weights,,200,,[]
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,230,ml_signal,54,Use of Xavier and orthogonal initialization for weights,,200,        self.b_fre = nn.Parameter(torch.ones(self.freq_dim)),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 65, 62, 19503, 796, 299, 77, 13, 36301, 7, 13165, 354, 13, 1952, 7, 944, 13, 19503, 80, 62, 27740, 4008]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,273,ml_signal,56,Use of Xavier and orthogonal initialization for weights,,230,"        self.W_c = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 54, 62, 66, 796, 299, 77, 13, 36301, 7, 15003, 13, 87, 19492, 62, 403, 6933, 41052, 13165, 354, 13, 28920, 7, 944, 13, 15414, 62, 27740, 11, 2116, 13, 30342, 62, 27740, 22305]"
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,273,ml_signal,59,Use of Xavier and orthogonal initialization for weights,,273,,[]
🧠 ML Signal: Use of Xavier and orthogonal initialization for weights,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 30825, 290, 29617, 519, 20996, 37588, 329, 19590]",1.0,303,ml_signal,62,Use of Xavier and orthogonal initialization for weights,,273,        self.b_o = nn.Parameter(torch.zeros(self.hidden_dim)),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 65, 62, 78, 796, 299, 77, 13, 36301, 7, 13165, 354, 13, 9107, 418, 7, 944, 13, 30342, 62, 27740, 4008]"
✅ Best Practice: Use of nn.Linear for defining fully connected layers,"[26486, 227, 6705, 19939, 25, 5765, 286, 299, 77, 13, 14993, 451, 329, 16215, 3938, 5884, 11685]",0.5,333,best_practice,68,Use of nn.Linear for defining fully connected layers,,303,        self.b_p = nn.Parameter(torch.zeros(self.output_dim)),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 65, 62, 79, 796, 299, 77, 13, 36301, 7, 13165, 354, 13, 9107, 418, 7, 944, 13, 22915, 62, 27740, 4008]"
🧠 ML Signal: Reshaping input data is a common preprocessing step in ML models,"[8582, 100, 254, 10373, 26484, 25, 1874, 71, 9269, 5128, 1366, 318, 257, 2219, 662, 36948, 2239, 287, 10373, 4981]",1.0,333,ml_signal,59,Reshaping input data is a common preprocessing step in ML models,,333,,[]
🧠 ML Signal: Permuting dimensions is often used in sequence models,"[8582, 100, 254, 10373, 26484, 25, 2448, 76, 15129, 15225, 318, 1690, 973, 287, 8379, 4981]",1.0,374,ml_signal,61,Permuting dimensions is often used in sequence models,,333,"        self.U_o = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 52, 62, 78, 796, 299, 77, 13, 36301, 7, 15003, 13, 1506, 519, 20996, 41052, 13165, 354, 13, 28920, 7, 944, 13, 30342, 62, 27740, 11, 2116, 13, 30342, 62, 27740, 22305]"
🧠 ML Signal: Initializing states is typical in RNNs and LSTMs,"[8582, 100, 254, 10373, 26484, 25, 20768, 2890, 2585, 318, 7226, 287, 371, 6144, 82, 290, 406, 2257, 10128]",0.5,417,ml_signal,67,Initializing states is typical in RNNs and LSTMs,,374,"        self.W_p = nn.Parameter(init.xavier_uniform_(torch.empty(self.hidden_dim, self.output_dim)))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 54, 62, 79, 796, 299, 77, 13, 36301, 7, 15003, 13, 87, 19492, 62, 403, 6933, 41052, 13165, 354, 13, 28920, 7, 944, 13, 30342, 62, 27740, 11, 2116, 13, 22915, 62, 27740, 22305]"
🧠 ML Signal: Getting constants is a pattern in recurrent models,"[8582, 100, 254, 10373, 26484, 25, 18067, 38491, 318, 257, 3912, 287, 42465, 4981]",0.5,417,ml_signal,69,Getting constants is a pattern in recurrent models,,417,,[]
🧠 ML Signal: Matrix multiplication is a core operation in neural networks,"[8582, 100, 254, 10373, 26484, 25, 24936, 48473, 318, 257, 4755, 4905, 287, 17019, 7686]",1.0,446,ml_signal,79,Matrix multiplication is a core operation in neural networks,,417,"        input = input.permute(0, 2, 1)  # [N, T, F]","[220, 220, 220, 220, 220, 220, 220, 5128, 796, 5128, 13, 16321, 1133, 7, 15, 11, 362, 11, 352, 8, 220, 1303, 685, 45, 11, 309, 11, 376, 60]"
🧠 ML Signal: Activation functions are key components in neural networks,"[8582, 100, 254, 10373, 26484, 25, 13144, 341, 5499, 389, 1994, 6805, 287, 17019, 7686]",1.0,469,ml_signal,85,Activation functions are key components in neural networks,,446,                self.init_states(x),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 15003, 62, 27219, 7, 87, 8]"
✅ Best Practice: Reshaping tensors for compatibility in operations,"[26486, 227, 6705, 19939, 25, 1874, 71, 9269, 11192, 669, 329, 17764, 287, 4560]",1.0,493,best_practice,89,Reshaping tensors for compatibility in operations,,469,            S_re_tm1 = self.states[2],"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 311, 62, 260, 62, 17209, 16, 796, 2116, 13, 27219, 58, 17, 60]"
🧠 ML Signal: Use of trigonometric functions in signal processing models,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 5192, 261, 16996, 5499, 287, 6737, 7587, 4981]",1.0,493,ml_signal,95,Use of trigonometric functions in signal processing models,,493,,[]
✅ Best Practice: Reshaping tensors for compatibility in operations,"[26486, 227, 6705, 19939, 25, 1874, 71, 9269, 11192, 669, 329, 17764, 287, 4560]",1.0,534,best_practice,99,Reshaping tensors for compatibility in operations,,493,"            x_c = torch.matmul(x * B_W[0], self.W_c) + self.b_c","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2124, 62, 66, 796, 28034, 13, 6759, 76, 377, 7, 87, 1635, 347, 62, 54, 58, 15, 4357, 2116, 13, 54, 62, 66, 8, 1343, 2116, 13, 65, 62, 66]"
✅ Best Practice: Reshaping tensors for compatibility in operations,"[26486, 227, 6705, 19939, 25, 1874, 71, 9269, 11192, 669, 329, 17764, 287, 4560]",1.0,580,best_practice,104,Reshaping tensors for compatibility in operations,,534,"            fre = self.inner_activation(x_fre + torch.matmul(h_tm1 * B_U[0], self.U_fre))","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2030, 796, 2116, 13, 5083, 62, 48545, 7, 87, 62, 19503, 1343, 28034, 13, 6759, 76, 377, 7, 71, 62, 17209, 16, 1635, 347, 62, 52, 58, 15, 4357, 2116, 13, 52, 62, 19503, 4008]"
✅ Best Practice: Reshaping tensors for compatibility in operations,"[26486, 227, 6705, 19939, 25, 1874, 71, 9269, 11192, 669, 329, 17764, 287, 4560]",1.0,612,best_practice,107,Reshaping tensors for compatibility in operations,,580,"            fre = torch.reshape(fre, (-1, 1, self.freq_dim))","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2030, 796, 28034, 13, 3447, 1758, 7, 19503, 11, 13841, 16, 11, 352, 11, 2116, 13, 19503, 80, 62, 27740, 4008]"
🧠 ML Signal: Updating states is a common pattern in recurrent models,"[8582, 100, 254, 10373, 26484, 25, 3205, 38734, 2585, 318, 257, 2219, 3912, 287, 42465, 4981]",1.0,631,ml_signal,113,Updating states is a common pattern in recurrent models,,612,            time = time_tm1 + 1,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 640, 796, 640, 62, 17209, 16, 1343, 352]"
✅ Best Practice: Clearing states after processing,"[26486, 227, 6705, 19939, 25, 3779, 1723, 2585, 706, 7587]",1.0,659,best_practice,115,Clearing states after processing,,631,            omega = torch.tensor(2 * np.pi) * time * frequency,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37615, 796, 28034, 13, 83, 22854, 7, 17, 1635, 45941, 13, 14415, 8, 1635, 640, 1635, 8373]"
🧠 ML Signal: Fully connected layers are common in neural network outputs,"[8582, 100, 254, 10373, 26484, 25, 40234, 5884, 11685, 389, 2219, 287, 17019, 3127, 23862]",1.0,679,ml_signal,117,Fully connected layers are common in neural network outputs,,659,            re = torch.cos(omega),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 302, 796, 28034, 13, 6966, 7, 462, 4908, 8]"
"🧠 ML Signal: Storing initial states in a list for a model, indicating a pattern for stateful computations","[8582, 100, 254, 10373, 26484, 25, 520, 3255, 4238, 2585, 287, 257, 1351, 329, 257, 2746, 11, 12739, 257, 3912, 329, 1181, 913, 2653, 602]",0.5,699,ml_signal,117,"Storing initial states in a list for a model, indicating a pattern for stateful computations",,679,            re = torch.cos(omega),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 302, 796, 28034, 13, 6966, 7, 462, 4908, 8]"
✅ Best Practice: Using list comprehension for concise and efficient list creation,"[26486, 227, 6705, 19939, 25, 8554, 1351, 35915, 329, 35327, 290, 6942, 1351, 6282]",0.5,732,best_practice,129,Using list comprehension for concise and efficient list creation,,699,"            A_a = torch.reshape(A_a, (-1, self.hidden_dim))","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 317, 62, 64, 796, 28034, 13, 3447, 1758, 7, 32, 62, 64, 11, 13841, 16, 11, 2116, 13, 30342, 62, 27740, 4008]"
✅ Best Practice: Using list comprehension for concise and efficient list creation,"[26486, 227, 6705, 19939, 25, 8554, 1351, 35915, 329, 35327, 290, 6942, 1351, 6282]",0.5,732,best_practice,131,Using list comprehension for concise and efficient list creation,,732,,[]
✅ Best Practice: Using numpy for efficient numerical operations,"[26486, 227, 6705, 19939, 25, 8554, 299, 32152, 329, 6942, 29052, 4560]",0.5,732,best_practice,133,Using numpy for efficient numerical operations,,732,,[]
✅ Best Practice: Converting numpy array to torch tensor for compatibility with PyTorch operations,"[26486, 227, 6705, 19939, 25, 35602, 889, 299, 32152, 7177, 284, 28034, 11192, 273, 329, 17764, 351, 9485, 15884, 354, 4560]",0.5,765,best_practice,135,Converting numpy array to torch tensor for compatibility with PyTorch operations,,732,"            p = torch.matmul(h, self.W_p) + self.b_p","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 279, 796, 28034, 13, 6759, 76, 377, 7, 71, 11, 2116, 13, 54, 62, 79, 8, 1343, 2116, 13, 65, 62, 79]"
⚠️ SAST Risk (Low): Potential risk if self.states is not properly initialized or if index 5 is out of bounds,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 2526, 611, 2116, 13, 27219, 318, 407, 6105, 23224, 393, 611, 6376, 642, 318, 503, 286, 22303]",0.5,798,sast_risk,135,Potential risk if self.states is not properly initialized or if index 5 is out of bounds,Low,765,"            p = torch.matmul(h, self.W_p) + self.b_p","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 279, 796, 28034, 13, 6759, 76, 377, 7, 71, 11, 2116, 13, 54, 62, 79, 8, 1343, 2116, 13, 65, 62, 79]"
"🧠 ML Signal: Defines a machine learning model class with parameters, useful for model architecture learning","[8582, 100, 254, 10373, 26484, 25, 2896, 1127, 257, 4572, 4673, 2746, 1398, 351, 10007, 11, 4465, 329, 2746, 10959, 4673]",0.5,814,ml_signal,134,"Defines a machine learning model class with parameters, useful for model architecture learning",,798,            h = o * a,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 289, 796, 267, 1635, 257]"
🧠 ML Signal: Logging initialization and parameters can be used to understand model configuration patterns,"[8582, 100, 254, 10373, 26484, 25, 5972, 2667, 37588, 290, 10007, 460, 307, 973, 284, 1833, 2746, 8398, 7572]",1.0,814,ml_signal,169,Logging initialization and parameters can be used to understand model configuration patterns,,814,,[]
🧠 ML Signal: Model configuration parameters are set as instance variables,"[8582, 100, 254, 10373, 26484, 25, 9104, 8398, 10007, 389, 900, 355, 4554, 9633]",0.5,849,ml_signal,172,Model configuration parameters are set as instance variables,,814,        constants.append([torch.tensor(1.0).to(self.device) for _ in range(6)]),"[220, 220, 220, 220, 220, 220, 220, 38491, 13, 33295, 26933, 13165, 354, 13, 83, 22854, 7, 16, 13, 15, 737, 1462, 7, 944, 13, 25202, 8, 329, 4808, 287, 2837, 7, 21, 8, 12962]"
✅ Best Practice: Convert optimizer to lowercase to ensure consistent comparison,"[26486, 227, 6705, 19939, 25, 38240, 6436, 7509, 284, 2793, 7442, 284, 4155, 6414, 7208]",1.0,857,best_practice,185,Convert optimizer to lowercase to ensure consistent comparison,,849,    input_dim : int,"[220, 220, 220, 5128, 62, 27740, 1058, 493]"
⚠️ SAST Risk (Low): Potential GPU index out of range if GPU is not available,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 11362, 6376, 503, 286, 2837, 611, 11362, 318, 407, 1695]",1.0,865,sast_risk,187,Potential GPU index out of range if GPU is not available,Low,857,    output_dim : int,"[220, 220, 220, 5072, 62, 27740, 1058, 493]"
🧠 ML Signal: Logging detailed model parameters for traceability,"[8582, 100, 254, 10373, 26484, 25, 5972, 2667, 6496, 2746, 10007, 329, 12854, 1799]",1.0,873,ml_signal,187,Logging detailed model parameters for traceability,,865,    output_dim : int,"[220, 220, 220, 5072, 62, 27740, 1058, 493]"
"⚠️ SAST Risk (Low): Seed setting for reproducibility, but may not cover all sources of randomness","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 23262, 4634, 329, 8186, 66, 2247, 11, 475, 743, 407, 3002, 477, 4237, 286, 4738, 1108]",0.5,893,sast_risk,228,"Seed setting for reproducibility, but may not cover all sources of randomness",Low,873,        self.n_epochs = n_epochs,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 77, 62, 538, 5374, 82, 796, 299, 62, 538, 5374, 82]"
🧠 ML Signal: Instantiation of the model with configuration parameters,"[8582, 100, 254, 10373, 26484, 25, 24470, 3920, 286, 262, 2746, 351, 8398, 10007]",0.5,905,ml_signal,237,Instantiation of the model with configuration parameters,,893,        self.seed = seed,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 28826, 796, 9403]"
🧠 ML Signal: Logging model architecture and size for analysis,"[8582, 100, 254, 10373, 26484, 25, 5972, 2667, 2746, 10959, 290, 2546, 329, 3781]",0.5,924,ml_signal,244,Logging model architecture and size for analysis,,905,"            ""\nfrequency_dimension : {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 35324, 62, 46156, 1058, 23884, 1]"
✅ Best Practice: Use of conditional logic to select optimizer,"[26486, 227, 6705, 19939, 25, 5765, 286, 26340, 9156, 284, 2922, 6436, 7509]",1.0,944,best_practice,246,Use of conditional logic to select optimizer,,924,"            ""\ndropout_U: {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 358, 1773, 448, 62, 52, 25, 23884, 1]"
⚠️ SAST Risk (Low): Use of NotImplementedError to handle unsupported optimizers,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 5765, 286, 1892, 3546, 1154, 12061, 12331, 284, 5412, 24222, 6436, 11341]",0.5,962,sast_risk,253,Use of NotImplementedError to handle unsupported optimizers,Low,944,"            ""\noptimizer : {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 40085, 7509, 1058, 23884, 1]"
🧠 ML Signal: Model is moved to the specified device (CPU/GPU),"[8582, 100, 254, 10373, 26484, 25, 9104, 318, 3888, 284, 262, 7368, 3335, 357, 36037, 14, 33346, 8]",0.5,981,ml_signal,256,Model is moved to the specified device (CPU/GPU),,962,"            ""\nuse_GPU : {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 1904, 62, 33346, 1058, 23884, 1]"
🧠 ML Signal: Checking if a GPU is being used for computation,"[8582, 100, 254, 10373, 26484, 25, 39432, 611, 257, 11362, 318, 852, 973, 329, 29964]",0.5,998,ml_signal,248,Checking if a GPU is being used for computation,,981,"            ""\nlr : {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 14050, 1058, 23884, 1]"
✅ Best Practice: Using torch.device to handle device types,"[26486, 227, 6705, 19939, 25, 8554, 28034, 13, 25202, 284, 5412, 3335, 3858]",1.0,1017,best_practice,250,Using torch.device to handle device types,,998,"            ""\nbatch_size : {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 43501, 62, 7857, 1058, 23884, 1]"
✅ Best Practice: Set the model to evaluation mode to disable dropout and batch normalization,"[26486, 227, 6705, 19939, 25, 5345, 262, 2746, 284, 12660, 4235, 284, 15560, 4268, 448, 290, 15458, 3487, 1634]",1.0,1035,best_practice,253,Set the model to evaluation mode to disable dropout and batch normalization,,1017,"            ""\noptimizer : {}""","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 40085, 7509, 1058, 23884, 1]"
🧠 ML Signal: Use of indices for batching indicates a custom batching mechanism,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 36525, 329, 15458, 278, 9217, 257, 2183, 15458, 278, 9030]",0.5,1054,ml_signal,257,Use of indices for batching indicates a custom batching mechanism,,1035,"            ""\nseed : {}"".format(","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 37082, 77, 28826, 1058, 23884, 1911, 18982, 7]"
⚠️ SAST Risk (Low): Potential for device mismatch if self.device is not set correctly,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 329, 3335, 46318, 611, 2116, 13, 25202, 318, 407, 900, 9380]",0.5,1074,sast_risk,262,Potential for device mismatch if self.device is not set correctly,Low,1054,"                dropout_W,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 4268, 448, 62, 54, 11]"
⚠️ SAST Risk (Low): Potential for device mismatch if self.device is not set correctly,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 329, 3335, 46318, 611, 2116, 13, 25202, 318, 407, 900, 9380]",0.5,1095,sast_risk,264,Potential for device mismatch if self.device is not set correctly,Low,1074,"                n_epochs,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 299, 62, 538, 5374, 82, 11]"
🧠 ML Signal: Model prediction step,"[8582, 100, 254, 10373, 26484, 25, 9104, 17724, 2239]",0.5,1112,ml_signal,266,Model prediction step,,1095,"                metric,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 18663, 11]"
🧠 ML Signal: Loss calculation step,"[8582, 100, 254, 10373, 26484, 25, 22014, 17952, 2239]",0.5,1131,ml_signal,268,Loss calculation step,,1112,"                early_stop,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1903, 62, 11338, 11]"
✅ Best Practice: Use .item() to convert a single-valued tensor to a Python number,"[26486, 227, 6705, 19939, 25, 5765, 764, 9186, 3419, 284, 10385, 257, 2060, 12, 39728, 11192, 273, 284, 257, 11361, 1271]",1.0,1151,best_practice,270,Use .item() to convert a single-valued tensor to a Python number,,1131,"                optimizer.lower(),","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 6436, 7509, 13, 21037, 22784]"
🧠 ML Signal: Metric calculation step,"[8582, 100, 254, 10373, 26484, 25, 3395, 1173, 17952, 2239]",0.5,1170,ml_signal,272,Metric calculation step,,1151,"                self.device,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 25202, 11]"
✅ Best Practice: Use .item() to convert a single-valued tensor to a Python number,"[26486, 227, 6705, 19939, 25, 5765, 764, 9186, 3419, 284, 10385, 257, 2060, 12, 39728, 11192, 273, 284, 257, 11361, 1271]",1.0,1187,best_practice,274,Use .item() to convert a single-valued tensor to a Python number,,1170,"                seed,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 9403, 11]"
✅ Best Practice: Return the mean of losses and scores for better interpretability,"[26486, 227, 6705, 19939, 25, 8229, 262, 1612, 286, 9089, 290, 8198, 329, 1365, 6179, 1799]",1.0,1195,best_practice,276,Return the mean of losses and scores for better interpretability,,1187,        ),"[220, 220, 220, 220, 220, 220, 220, 1267]"
🧠 ML Signal: Use of .values to extract numpy arrays from pandas DataFrames,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 764, 27160, 284, 7925, 299, 32152, 26515, 422, 19798, 292, 6060, 35439]",0.5,1214,ml_signal,269,Use of .values to extract numpy arrays from pandas DataFrames,,1195,"                eval_steps,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5418, 62, 20214, 11]"
🧠 ML Signal: Use of np.squeeze to remove single-dimensional entries from the shape of an array,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 45941, 13, 16485, 1453, 2736, 284, 4781, 2060, 12, 19577, 12784, 422, 262, 5485, 286, 281, 7177]",0.5,1231,ml_signal,271,Use of np.squeeze to remove single-dimensional entries from the shape of an array,,1214,"                loss,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2994, 11]"
🧠 ML Signal: Setting model to training mode,"[8582, 100, 254, 10373, 26484, 25, 25700, 2746, 284, 3047, 4235]",1.0,1252,ml_signal,273,Setting model to training mode,,1231,"                self.use_gpu,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 1904, 62, 46999, 11]"
🧠 ML Signal: Use of np.arange to create an array of indices,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 45941, 13, 283, 858, 284, 2251, 281, 7177, 286, 36525]",0.5,1264,ml_signal,275,Use of np.arange to create an array of indices,,1252,            ),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1267]"
🧠 ML Signal: Shuffling data indices for stochastic gradient descent,"[8582, 100, 254, 10373, 26484, 25, 911, 1648, 1359, 1366, 36525, 329, 3995, 354, 3477, 31312, 18598]",1.0,1264,ml_signal,277,Shuffling data indices for stochastic gradient descent,,1264,,[]
🧠 ML Signal: Conversion of numpy arrays to PyTorch tensors and moving to device,"[8582, 100, 254, 10373, 26484, 25, 44101, 286, 299, 32152, 26515, 284, 9485, 15884, 354, 11192, 669, 290, 3867, 284, 3335]",0.5,1283,ml_signal,282,Conversion of numpy arrays to PyTorch tensors and moving to device,,1264,        self.sfm_model = SFM_Model(,"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 28202, 76, 62, 19849, 796, 14362, 44, 62, 17633, 7]"
🧠 ML Signal: Conversion of numpy arrays to PyTorch tensors and moving to device,"[8582, 100, 254, 10373, 26484, 25, 44101, 286, 299, 32152, 26515, 284, 9485, 15884, 354, 11192, 669, 290, 3867, 284, 3335]",0.5,1304,ml_signal,284,Conversion of numpy arrays to PyTorch tensors and moving to device,,1283,"            output_dim=self.output_dim,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5072, 62, 27740, 28, 944, 13, 22915, 62, 27740, 11]"
🧠 ML Signal: Forward pass through the model,"[8582, 100, 254, 10373, 26484, 25, 19530, 1208, 832, 262, 2746]",0.5,1325,ml_signal,285,Forward pass through the model,,1304,"            hidden_size=self.hidden_size,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 7104, 62, 7857, 28, 944, 13, 30342, 62, 7857, 11]"
🧠 ML Signal: Calculation of loss using a loss function,"[8582, 100, 254, 10373, 26484, 25, 2199, 14902, 286, 2994, 1262, 257, 2994, 2163]",0.5,1346,ml_signal,285,Calculation of loss using a loss function,,1325,"            hidden_size=self.hidden_size,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 7104, 62, 7857, 28, 944, 13, 30342, 62, 7857, 11]"
🧠 ML Signal: Zeroing gradients before backward pass,"[8582, 100, 254, 10373, 26484, 25, 12169, 278, 3915, 2334, 878, 19528, 1208]",0.5,1376,ml_signal,291,Zeroing gradients before backward pass,,1346,"        self.logger.info(""model:\n{:}"".format(self.sfm_model))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 19849, 7479, 77, 90, 25, 92, 1911, 18982, 7, 944, 13, 28202, 76, 62, 19849, 4008]"
🧠 ML Signal: Backward pass for gradient computation,"[8582, 100, 254, 10373, 26484, 25, 5157, 904, 1208, 329, 31312, 29964]",0.5,1406,ml_signal,291,Backward pass for gradient computation,,1376,"        self.logger.info(""model:\n{:}"".format(self.sfm_model))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 19849, 7479, 77, 90, 25, 92, 1911, 18982, 7, 944, 13, 28202, 76, 62, 19849, 4008]"
⚠️ SAST Risk (Low): Potential for gradient explosion without proper clipping,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 329, 31312, 11278, 1231, 1774, 45013]",1.0,1424,sast_risk,296,Potential for gradient explosion without proper clipping,Low,1406,"        elif optimizer.lower() == ""gd"":","[220, 220, 220, 220, 220, 220, 220, 1288, 361, 6436, 7509, 13, 21037, 3419, 6624, 366, 21287, 1298]"
🧠 ML Signal: Optimizer step to update model parameters,"[8582, 100, 254, 10373, 26484, 25, 30011, 7509, 2239, 284, 4296, 2746, 10007]",1.0,1442,ml_signal,296,Optimizer step to update model parameters,,1424,"        elif optimizer.lower() == ""gd"":","[220, 220, 220, 220, 220, 220, 220, 1288, 361, 6436, 7509, 13, 21037, 3419, 6624, 366, 21287, 1298]"
⚠️ SAST Risk (Low): Potential mutable default argument for evals_result,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 4517, 540, 4277, 4578, 329, 819, 874, 62, 20274]",0.5,1472,sast_risk,291,Potential mutable default argument for evals_result,Low,1442,"        self.logger.info(""model:\n{:}"".format(self.sfm_model))","[220, 220, 220, 220, 220, 220, 220, 2116, 13, 6404, 1362, 13, 10951, 7203, 19849, 7479, 77, 90, 25, 92, 1911, 18982, 7, 944, 13, 28202, 76, 62, 19849, 4008]"
⚠️ SAST Risk (Low): Potential resource management issue if not using 'cpu',"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 8271, 4542, 2071, 611, 407, 1262, 705, 36166, 6]",0.5,1472,sast_risk,333,Potential resource management issue if not using 'cpu',Low,1472,,[]
"🧠 ML Signal: Function for calculating mean squared error, a common loss function in regression tasks","[8582, 100, 254, 10373, 26484, 25, 15553, 329, 26019, 1612, 44345, 4049, 11, 257, 2219, 2994, 2163, 287, 20683, 8861]",1.0,1494,ml_signal,334,"Function for calculating mean squared error, a common loss function in regression tasks",,1472,"        return np.mean(losses), np.mean(scores)","[220, 220, 220, 220, 220, 220, 220, 1441, 45941, 13, 32604, 7, 22462, 274, 828, 45941, 13, 32604, 7, 1416, 2850, 8]"
✅ Best Practice: Use of descriptive variable names like 'pred' and 'label' for clarity,"[26486, 227, 6705, 19939, 25, 5765, 286, 35644, 7885, 3891, 588, 705, 28764, 6, 290, 705, 18242, 6, 329, 16287]",1.0,1513,best_practice,336,Use of descriptive variable names like 'pred' and 'label' for clarity,,1494,"    def train_epoch(self, x_train, y_train):","[220, 220, 220, 825, 4512, 62, 538, 5374, 7, 944, 11, 2124, 62, 27432, 11, 331, 62, 27432, 2599]"
⚠️ SAST Risk (Low): Assumes 'pred' and 'label' are tensors; lacks input validation,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 2195, 8139, 705, 28764, 6, 290, 705, 18242, 6, 389, 11192, 669, 26, 16523, 5128, 21201]",1.0,1538,sast_risk,338,Assumes 'pred' and 'label' are tensors; lacks input validation,Low,1513,        y_train_values = np.squeeze(y_train.values),"[220, 220, 220, 220, 220, 220, 220, 331, 62, 27432, 62, 27160, 796, 45941, 13, 16485, 1453, 2736, 7, 88, 62, 27432, 13, 27160, 8]"
"🧠 ML Signal: Use of torch.mean, indicating integration with PyTorch for tensor operations","[8582, 100, 254, 10373, 26484, 25, 5765, 286, 28034, 13, 32604, 11, 12739, 11812, 351, 9485, 15884, 354, 329, 11192, 273, 4560]",0.5,1554,ml_signal,340,"Use of torch.mean, indicating integration with PyTorch for tensor operations",,1538,        self.sfm_model.train(),"[220, 220, 220, 220, 220, 220, 220, 2116, 13, 28202, 76, 62, 19849, 13, 27432, 3419]"
✅ Best Practice: Consider adding type hints for function parameters and return type,"[26486, 227, 6705, 19939, 25, 12642, 4375, 2099, 20269, 329, 2163, 10007, 290, 1441, 2099]",0.5,1572,best_practice,337,Consider adding type hints for function parameters and return type,,1554,        x_train_values = x_train.values,"[220, 220, 220, 220, 220, 220, 220, 2124, 62, 27432, 62, 27160, 796, 2124, 62, 27432, 13, 27160]"
🧠 ML Signal: Usage of torch.isnan to handle missing values in labels,"[8582, 100, 254, 10373, 26484, 25, 29566, 286, 28034, 13, 271, 12647, 284, 5412, 4814, 3815, 287, 14722]",0.5,1572,ml_signal,339,Usage of torch.isnan to handle missing values in labels,,1572,,[]
🧠 ML Signal: Conditional logic based on self.loss attribute,"[8582, 100, 254, 10373, 26484, 25, 9724, 1859, 9156, 1912, 319, 2116, 13, 22462, 11688]",1.0,1572,ml_signal,341,Conditional logic based on self.loss attribute,,1572,,[]
🧠 ML Signal: Usage of mask to filter predictions and labels,"[8582, 100, 254, 10373, 26484, 25, 29566, 286, 9335, 284, 8106, 16277, 290, 14722]",0.5,1589,ml_signal,343,Usage of mask to filter predictions and labels,,1572,        np.random.shuffle(indices),"[220, 220, 220, 220, 220, 220, 220, 45941, 13, 25120, 13, 1477, 18137, 7, 521, 1063, 8]"
"⚠️ SAST Risk (Low): Potential for unhandled exception if self.loss is not ""mse""","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 329, 555, 38788, 6631, 611, 2116, 13, 22462, 318, 407, 366, 76, 325, 1]",0.5,1614,sast_risk,345,"Potential for unhandled exception if self.loss is not ""mse""",Low,1589,        for i in range(len(indices))[:: self.batch_size]:,"[220, 220, 220, 220, 220, 220, 220, 329, 1312, 287, 2837, 7, 11925, 7, 521, 1063, 4008, 58, 3712, 2116, 13, 43501, 62, 7857, 5974]"
✅ Best Practice: Consider adding type hints for function parameters and return type for better readability and maintainability.,"[26486, 227, 6705, 19939, 25, 12642, 4375, 2099, 20269, 329, 2163, 10007, 290, 1441, 2099, 329, 1365, 1100, 1799, 290, 5529, 1799, 13]",1.0,1636,best_practice,342,Consider adding type hints for function parameters and return type for better readability and maintainability.,,1614,        indices = np.arange(len(x_train_values)),"[220, 220, 220, 220, 220, 220, 220, 36525, 796, 45941, 13, 283, 858, 7, 11925, 7, 87, 62, 27432, 62, 27160, 4008]"
🧠 ML Signal: Use of torch.isfinite to create a mask for valid (finite) values in the label tensor.,"[8582, 100, 254, 10373, 26484, 25, 5765, 286, 28034, 13, 4468, 9504, 284, 2251, 257, 9335, 329, 4938, 357, 69, 9504, 8, 3815, 287, 262, 6167, 11192, 273, 13]",0.5,1636,ml_signal,344,Use of torch.isfinite to create a mask for valid (finite) values in the label tensor.,,1636,,[]
"🧠 ML Signal: Conditional logic based on the value of self.metric, indicating dynamic behavior based on configuration.","[8582, 100, 254, 10373, 26484, 25, 9724, 1859, 9156, 1912, 319, 262, 1988, 286, 2116, 13, 4164, 1173, 11, 12739, 8925, 4069, 1912, 319, 8398, 13]",0.5,1662,ml_signal,346,"Conditional logic based on the value of self.metric, indicating dynamic behavior based on configuration.",,1636,            if len(indices) - i < self.batch_size:,"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 611, 18896, 7, 521, 1063, 8, 532, 1312, 1279, 2116, 13, 43501, 62, 7857, 25]"
"⚠️ SAST Risk (Low): Potential risk if self.loss_fn is not properly validated or sanitized, leading to unexpected behavior.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 2526, 611, 2116, 13, 22462, 62, 22184, 318, 407, 6105, 31031, 393, 5336, 36951, 11, 3756, 284, 10059, 4069, 13]",0.5,1662,sast_risk,348,"Potential risk if self.loss_fn is not properly validated or sanitized, leading to unexpected behavior.",Low,1662,,[]
"⚠️ SAST Risk (Low): Use of string formatting with user-controlled input in exception message, though risk is minimal here.","[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 5765, 286, 4731, 33313, 351, 2836, 12, 14401, 5128, 287, 6631, 3275, 11, 996, 2526, 318, 10926, 994, 13]",0.5,1710,sast_risk,350,"Use of string formatting with user-controlled input in exception message, though risk is minimal here.",Low,1662,            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 6167, 796, 28034, 13, 6738, 62, 77, 32152, 7, 88, 62, 27432, 62, 27160, 58, 521, 1063, 58, 72, 1058, 1312, 1343, 2116, 13, 43501, 62, 7857, 11907, 737, 22468, 22446, 1462, 7, 944, 13, 25202, 8]"
⚠️ SAST Risk (Low): Potential exception if 'self.fitted' is not a boolean,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 6631, 611, 705, 944, 13, 38631, 6, 318, 407, 257, 25131]",0.5,1710,sast_risk,348,Potential exception if 'self.fitted' is not a boolean,Low,1710,,[]
🧠 ML Signal: Usage of dataset preparation for prediction,"[8582, 100, 254, 10373, 26484, 25, 29566, 286, 27039, 11824, 329, 17724]",1.0,1710,ml_signal,351,Usage of dataset preparation for prediction,,1710,,[]
🧠 ML Signal: Model evaluation mode set before prediction,"[8582, 100, 254, 10373, 26484, 25, 9104, 12660, 4235, 900, 878, 17724]",1.0,1710,ml_signal,354,Model evaluation mode set before prediction,,1710,,[]
✅ Best Practice: Use of range with step for batch processing,"[26486, 227, 6705, 19939, 25, 5765, 286, 2837, 351, 2239, 329, 15458, 7587]",0.5,1730,best_practice,358,Use of range with step for batch processing,,1710,            self.train_optimizer.step(),"[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 2116, 13, 27432, 62, 40085, 7509, 13, 9662, 3419]"
⚠️ SAST Risk (Low): Potential device mismatch if 'self.device' is not set correctly,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 3335, 46318, 611, 705, 944, 13, 25202, 6, 318, 407, 900, 9380]",1.0,1734,sast_risk,365,Potential device mismatch if 'self.device' is not set correctly,Low,1730,    ):,"[220, 220, 220, 15179]"
✅ Best Practice: Use of 'torch.no_grad()' for inference to save memory,"[26486, 227, 6705, 19939, 25, 5765, 286, 705, 13165, 354, 13, 3919, 62, 9744, 3419, 6, 329, 32278, 284, 3613, 4088]",0.5,1751,best_practice,367,Use of 'torch.no_grad()' for inference to save memory,,1734,"            [""train"", ""valid""],","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 14631, 27432, 1600, 366, 12102, 33116]"
🧠 ML Signal: Model prediction and conversion to numpy,"[8582, 100, 254, 10373, 26484, 25, 9104, 17724, 290, 11315, 284, 299, 32152]",0.5,1774,ml_signal,369,Model prediction and conversion to numpy,,1751,"            data_key=DataHandlerLP.DK_L,","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 1366, 62, 2539, 28, 6601, 25060, 19930, 13, 48510, 62, 43, 11]"
🧠 ML Signal: Returning predictions as a pandas Series,"[8582, 100, 254, 10373, 26484, 25, 42882, 16277, 355, 257, 19798, 292, 7171]",0.5,1800,ml_signal,372,Returning predictions as a pandas Series,,1774,"            raise ValueError(""Empty data from dataset, please check your dataset config."")","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 5298, 11052, 12331, 7203, 40613, 1366, 422, 27039, 11, 3387, 2198, 534, 27039, 4566, 19570]"
✅ Best Practice: Class docstring provides a brief description of the class purpose,"[26486, 227, 6705, 19939, 25, 5016, 2205, 8841, 3769, 257, 4506, 6764, 286, 262, 1398, 4007]",1.0,1821,best_practice,368,Class docstring provides a brief description of the class purpose,,1800,"            col_set=[""feature"", ""label""],","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 951, 62, 2617, 28, 14692, 30053, 1600, 366, 18242, 33116]"
✅ Best Practice: Use of a constructor method to initialize an object,"[26486, 227, 6705, 19939, 25, 5765, 286, 257, 23772, 2446, 284, 41216, 281, 2134]",0.5,1842,best_practice,368,Use of a constructor method to initialize an object,,1821,"            col_set=[""feature"", ""label""],","[220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 951, 62, 2617, 28, 14692, 30053, 1600, 366, 18242, 33116]"
✅ Best Practice: Encapsulating initialization logic in a separate method,"[26486, 227, 6705, 19939, 25, 14711, 1686, 8306, 37588, 9156, 287, 257, 4553, 2446]",0.5,1850,best_practice,370,Encapsulating initialization logic in a separate method,,1842,        ),"[220, 220, 220, 220, 220, 220, 220, 1267]"
✅ Best Practice: Initializing or resetting instance variables to default values,"[26486, 227, 6705, 19939, 25, 20768, 2890, 393, 13259, 889, 4554, 9633, 284, 4277, 3815]",1.0,1870,best_practice,371,Initializing or resetting instance variables to default values,,1850,        if df_train.empty or df_valid.empty:,"[220, 220, 220, 220, 220, 220, 220, 611, 47764, 62, 27432, 13, 28920, 393, 47764, 62, 12102, 13, 28920, 25]"
✅ Best Practice: Initializing or resetting instance variables to default values,"[26486, 227, 6705, 19939, 25, 20768, 2890, 393, 13259, 889, 4554, 9633, 284, 4277, 3815]",1.0,1897,best_practice,373,Initializing or resetting instance variables to default values,,1870,"        x_train, y_train = df_train[""feature""], df_train[""label""]","[220, 220, 220, 220, 220, 220, 220, 2124, 62, 27432, 11, 331, 62, 27432, 796, 47764, 62, 27432, 14692, 30053, 33116, 47764, 62, 27432, 14692, 18242, 8973]"
✅ Best Practice: Initializing or resetting instance variables to default values,"[26486, 227, 6705, 19939, 25, 20768, 2890, 393, 13259, 889, 4554, 9633, 284, 4277, 3815]",1.0,1897,best_practice,375,Initializing or resetting instance variables to default values,,1897,,[]
✅ Best Practice: Initializing or resetting instance variables to default values,"[26486, 227, 6705, 19939, 25, 20768, 2890, 393, 13259, 889, 4554, 9633, 284, 4277, 3815]",1.0,1909,best_practice,377,Initializing or resetting instance variables to default values,,1897,        stop_steps = 0,"[220, 220, 220, 220, 220, 220, 220, 2245, 62, 20214, 796, 657]"
✅ Best Practice: Consider adding type hints for function parameters and return type,"[26486, 227, 6705, 19939, 25, 12642, 4375, 2099, 20269, 329, 2163, 10007, 290, 1441, 2099]",1.0,1932,best_practice,376,Consider adding type hints for function parameters and return type,,1909,        save_path = get_or_create_path(save_path),"[220, 220, 220, 220, 220, 220, 220, 3613, 62, 6978, 796, 651, 62, 273, 62, 17953, 62, 6978, 7, 21928, 62, 6978, 8]"
🧠 ML Signal: Tracking cumulative sum and count for average calculation,"[8582, 100, 254, 10373, 26484, 25, 37169, 23818, 2160, 290, 954, 329, 2811, 17952]",1.0,1944,ml_signal,378,Tracking cumulative sum and count for average calculation,,1932,        train_loss = 0,"[220, 220, 220, 220, 220, 220, 220, 4512, 62, 22462, 796, 657]"
🧠 ML Signal: Incremental update pattern for count,"[8582, 100, 254, 10373, 26484, 25, 10791, 37098, 4296, 3912, 329, 954]",1.0,1959,ml_signal,379,Incremental update pattern for count,,1944,        best_score = -np.inf,"[220, 220, 220, 220, 220, 220, 220, 1266, 62, 26675, 796, 532, 37659, 13, 10745]"
⚠️ SAST Risk (Low): Potential division by zero if self.count is zero,"[158, 248, 254, 37929, 311, 11262, 19602, 357, 20535, 2599, 32480, 7297, 416, 6632, 611, 2116, 13, 9127, 318, 6632]",1.0,1974,sast_risk,379,Potential division by zero if self.count is zero,Low,1959,        best_score = -np.inf,"[220, 220, 220, 220, 220, 220, 220, 1266, 62, 26675, 796, 532, 37659, 13, 10745]"
