[
  {
    "line": 5,
    "text": "except ModuleNotFoundError:",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues.",
    "confidence": 1.0,
    "tokens": [
      16341,
      19937,
      3673,
      21077,
      12331,
      25
    ],
    "start_token": 0,
    "end_token": 6,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      327,
      19775,
      3154,
      13269,
      588,
      19937,
      3673,
      21077,
      12331,
      1231,
      2176,
      9041,
      460,
      7808,
      584,
      2428,
      13
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues."
  },
  {
    "line": 6,
    "text": "    CatBoostModel = None",
    "annotation": "\u2705 Best Practice: Consider logging the error instead of printing to standard output.",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      5181,
      45686,
      17633,
      796,
      6045
    ],
    "start_token": 6,
    "end_token": 14,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      12642,
      18931,
      262,
      4049,
      2427,
      286,
      13570,
      284,
      3210,
      5072,
      13
    ],
    "label": "best_practice",
    "reason": "Consider logging the error instead of printing to standard output."
  },
  {
    "line": 14,
    "text": "        \"ModuleNotFoundError. DEnsembleModel and LGBModel are skipped. (optional: maybe installing lightgbm can fix it.)\"",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      366,
      26796,
      3673,
      21077,
      12331,
      13,
      360,
      4834,
      15140,
      17633,
      290,
      406,
      4579,
      17633,
      389,
      26684,
      13,
      357,
      25968,
      25,
      3863,
      15975,
      1657,
      70,
      20475,
      460,
      4259,
      340,
      2014,
      1
    ],
    "start_token": 14,
    "end_token": 51,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      327,
      19775,
      3154,
      13269,
      588,
      19937,
      3673,
      21077,
      12331,
      1231,
      2176,
      9041,
      460,
      7808,
      584,
      2428,
      13
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues."
  },
  {
    "line": 14,
    "text": "        \"ModuleNotFoundError. DEnsembleModel and LGBModel are skipped. (optional: maybe installing lightgbm can fix it.)\"",
    "annotation": "\u2705 Best Practice: Consider logging the error instead of printing to standard output.",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      366,
      26796,
      3673,
      21077,
      12331,
      13,
      360,
      4834,
      15140,
      17633,
      290,
      406,
      4579,
      17633,
      389,
      26684,
      13,
      357,
      25968,
      25,
      3863,
      15975,
      1657,
      70,
      20475,
      460,
      4259,
      340,
      2014,
      1
    ],
    "start_token": 51,
    "end_token": 88,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      12642,
      18931,
      262,
      4049,
      2427,
      286,
      13570,
      284,
      3210,
      5072,
      13
    ],
    "label": "best_practice",
    "reason": "Consider logging the error instead of printing to standard output."
  },
  {
    "line": 22,
    "text": "    from .linear import LinearModel",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      422,
      764,
      29127,
      1330,
      44800,
      17633
    ],
    "start_token": 88,
    "end_token": 97,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      327,
      19775,
      3154,
      13269,
      588,
      19937,
      3673,
      21077,
      12331,
      1231,
      2176,
      9041,
      460,
      7808,
      584,
      2428,
      13
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues."
  },
  {
    "line": 23,
    "text": "except ModuleNotFoundError:",
    "annotation": "\u2705 Best Practice: Consider logging the error instead of printing to standard output.",
    "confidence": 0.5,
    "tokens": [
      16341,
      19937,
      3673,
      21077,
      12331,
      25
    ],
    "start_token": 97,
    "end_token": 103,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      12642,
      18931,
      262,
      4049,
      2427,
      286,
      13570,
      284,
      3210,
      5072,
      13
    ],
    "label": "best_practice",
    "reason": "Consider logging the error instead of printing to standard output."
  },
  {
    "line": 29,
    "text": "    from .pytorch_gats import GATs",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      422,
      764,
      9078,
      13165,
      354,
      62,
      70,
      1381,
      1330,
      402,
      1404,
      82
    ],
    "start_token": 103,
    "end_token": 118,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      327,
      19775,
      3154,
      13269,
      588,
      19937,
      3673,
      21077,
      12331,
      1231,
      2176,
      9041,
      460,
      7808,
      584,
      2428,
      13
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues."
  },
  {
    "line": 30,
    "text": "    from .pytorch_gru import GRU",
    "annotation": "\u2705 Best Practice: Consider logging the error instead of printing to standard output.",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      422,
      764,
      9078,
      13165,
      354,
      62,
      48929,
      1330,
      10863,
      52
    ],
    "start_token": 118,
    "end_token": 131,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      12642,
      18931,
      262,
      4049,
      2427,
      286,
      13570,
      284,
      3210,
      5072,
      13
    ],
    "label": "best_practice",
    "reason": "Consider logging the error instead of printing to standard output."
  },
  {
    "line": 38,
    "text": "    pytorch_classes = (ALSTM, GATs, GRU, LSTM, DNNModelPytorch, TabnetModel, SFM_Model, TCN, ADD)",
    "annotation": "\ud83e\udde0 ML Signal: Importing multiple PyTorch models indicates usage of deep learning frameworks.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      12972,
      13165,
      354,
      62,
      37724,
      796,
      357,
      1847,
      2257,
      44,
      11,
      402,
      1404,
      82,
      11,
      10863,
      52,
      11,
      406,
      2257,
      44,
      11,
      360,
      6144,
      17633,
      20519,
      13165,
      354,
      11,
      16904,
      3262,
      17633,
      11,
      14362,
      44,
      62,
      17633,
      11,
      17283,
      45,
      11,
      27841,
      8
    ],
    "start_token": 131,
    "end_token": 177,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      17267,
      278,
      3294,
      9485,
      15884,
      354,
      4981,
      9217,
      8748,
      286,
      2769,
      4673,
      29251,
      13
    ],
    "label": "ml_signal",
    "reason": "Importing multiple PyTorch models indicates usage of deep learning frameworks."
  },
  {
    "line": 38,
    "text": "    pytorch_classes = (ALSTM, GATs, GRU, LSTM, DNNModelPytorch, TabnetModel, SFM_Model, TCN, ADD)",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      12972,
      13165,
      354,
      62,
      37724,
      796,
      357,
      1847,
      2257,
      44,
      11,
      402,
      1404,
      82,
      11,
      10863,
      52,
      11,
      406,
      2257,
      44,
      11,
      360,
      6144,
      17633,
      20519,
      13165,
      354,
      11,
      16904,
      3262,
      17633,
      11,
      14362,
      44,
      62,
      17633,
      11,
      17283,
      45,
      11,
      27841,
      8
    ],
    "start_token": 177,
    "end_token": 223,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      327,
      19775,
      3154,
      13269,
      588,
      19937,
      3673,
      21077,
      12331,
      1231,
      2176,
      9041,
      460,
      7808,
      584,
      2428,
      13
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "Catching broad exceptions like ModuleNotFoundError without specific handling can hide other issues."
  },
  {
    "line": 38,
    "text": "    pytorch_classes = (ALSTM, GATs, GRU, LSTM, DNNModelPytorch, TabnetModel, SFM_Model, TCN, ADD)",
    "annotation": "\u2705 Best Practice: Consider logging the error instead of printing to standard output.",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      12972,
      13165,
      354,
      62,
      37724,
      796,
      357,
      1847,
      2257,
      44,
      11,
      402,
      1404,
      82,
      11,
      10863,
      52,
      11,
      406,
      2257,
      44,
      11,
      360,
      6144,
      17633,
      20519,
      13165,
      354,
      11,
      16904,
      3262,
      17633,
      11,
      14362,
      44,
      62,
      17633,
      11,
      17283,
      45,
      11,
      27841,
      8
    ],
    "start_token": 223,
    "end_token": 269,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      12642,
      18931,
      262,
      4049,
      2427,
      286,
      13570,
      284,
      3210,
      5072,
      13
    ],
    "label": "best_practice",
    "reason": "Consider logging the error instead of printing to standard output."
  },
  {
    "line": 38,
    "text": "    pytorch_classes = (ALSTM, GATs, GRU, LSTM, DNNModelPytorch, TabnetModel, SFM_Model, TCN, ADD)",
    "annotation": "\ud83e\udde0 ML Signal: Aggregating model classes into a single tuple for unified handling.",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      12972,
      13165,
      354,
      62,
      37724,
      796,
      357,
      1847,
      2257,
      44,
      11,
      402,
      1404,
      82,
      11,
      10863,
      52,
      11,
      406,
      2257,
      44,
      11,
      360,
      6144,
      17633,
      20519,
      13165,
      354,
      11,
      16904,
      3262,
      17633,
      11,
      14362,
      44,
      62,
      17633,
      11,
      17283,
      45,
      11,
      27841,
      8
    ],
    "start_token": 269,
    "end_token": 315,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      19015,
      2301,
      803,
      2746,
      6097,
      656,
      257,
      2060,
      46545,
      329,
      22706,
      9041,
      13
    ],
    "label": "ml_signal",
    "reason": "Aggregating model classes into a single tuple for unified handling."
  }
]