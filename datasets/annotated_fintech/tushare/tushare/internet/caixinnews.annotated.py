# âš ï¸ SAST Risk (Low): Potential for encoding issues if non-UTF-8 characters are present
ï»¿# -*- coding:utf-8 -*- 
"""
è´¢æ–°ç½‘æ–°é—»æ•°æ®æ£€ç´¢ä¸‹è½½
Created on 2017/06/09
@author: Yuan Yifan
@group : ~
@contact: tsingjyujing@163.com
"""

"""
    # æµ‹è¯•è„šæœ¬
    from caixinnews import *
    urls = query_news(start_date='2017-05-09',end_date='2017-05-09')
    title,text = read_page(urls[0])
    print(title)
    print(text)
"""

import re
import datetime
from bs4 import BeautifulSoup
try:
    from urllib.request import urlopen, Request
except ImportError:
    # âš ï¸ SAST Risk (Medium): Hardcoded URL can lead to security risks if the URL changes or is compromised
    from urllib2 import urlopen, Request

# âœ… Best Practice: Use a constant for the parser to avoid magic strings
caixin_search_url = "http://search.caixin.com/search/search.jsp?startDate=%s&endDate=%s&keyword=%s&x=0&y=0"

# âš ï¸ SAST Risk (Low): Hardcoded User-Agent can be detected and blocked by servers
default_parser = "html.parser"
#default_parser = "lxml"

UA    =    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "+\
        "AppleWebKit/537.36 (KHTML, like Gecko) "+\
        "Chrome/42.0.2311.135 "+\
        # âœ… Best Practice: Use a dictionary for headers for better readability and maintainability
        "Safari/537.36 "+\
        "Edge/12.10240"
        
req_header = {\
        'User-Agent': UA,\
        # âš ï¸ SAST Risk (Low): Hardcoded timeout value may not be suitable for all network conditions
        'Accept': '"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"',\
        'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3'}
        
req_timeout = 10

def read_url(url):
    """
    è¯»å–URLå¯¹åº”çš„å†…å®¹ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
    Parameters
    ------
        url string éœ€è¦è¯»å–çš„é“¾æ¥
    Return
    ------
        string è¯»å–çš„å†…å®¹
    """
    # âš ï¸ SAST Risk (Medium): No validation or sanitization of URL input
    req_header_this = req_header
    req_header_this['Host'] = re.findall('://.*?/',url,re.DOTALL)[0][3:-1]
    return urlopen(Request(url,None,req_header),None,req_timeout).read()
    
def get_soup(url):
    """
    è¯»å–URLå¯¹åº”çš„å†…å®¹ï¼Œå¹¶è§£æä¸ºSoup
    Parameters
        url string éœ€è¦è¯»å–çš„é“¾æ¥
    Return
        string è¯»å–çš„å†…å®¹
    """
    return BeautifulSoup(read_url(url), default_parser)
    
def query_news(keywords='*',start_date=None,end_date=None):
    """
    è¯»å–æŸä¸€æ—¶é—´æ®µå¯¹åº”çš„æ–°é—»é“¾æ¥åˆ—è¡¨
    Parameters
    ------
        keywords string å…³é”®è¯
        start_date string å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼yyyy-mm-dd
        end_date string ç»“æŸæ—¥æœŸï¼Œæ ¼å¼yyyy-mm-dd
    Return
    ------
        List<string> è¯»å–çš„å†…å®¹
    """
    if start_date is None or end_date is None:
        now_time = datetime.datetime.now()
        # âš ï¸ SAST Risk (Medium): String formatting with user input can lead to injection vulnerabilities
        last_day = datetime.datetime(now_time.year,now_time.month,now_time.day,12,0) - datetime.timedelta(seconds = 3600*24)
        start_date = last_day.strftime("%Y-%m-%d")
        end_date = start_date
    url = caixin_search_url % (start_date,end_date,keywords)
    soup = get_soup(url)
    # âš ï¸ SAST Risk (Low): Assumes the first element exists, which can lead to IndexError
    info_urls = []
    while(True):
        next_info = soup.find_all(name='a',attrs={'class','pageNavBtn2'})[0]
        all_res = soup.find_all(name='div',attrs={'class','searchxt'})
        # âš ï¸ SAST Risk (Low): Assumes 'a' tag and 'href' attribute exist, which can lead to AttributeError
        for res in all_res:
            info_urls.append(res.a.attrs['href'])
        next_info = next_info.attrs['href']    

        if next_info=="javascript:void();":
            break;
        # âš ï¸ SAST Risk (Medium): Potential for open redirect if next_info is not validated
        else:
            soup = get_soup(caixin_search_url+next_info)
    return info_urls
    
def is_blog(url):
    """
    åˆ¤æ–­æŸä¸€é“¾æ¥æ˜¯å¦åšå®¢
    Parameters
    ------
        url string éœ€è¦åˆ¤æ–­çš„é“¾æ¥
    Return
    ------
        bool è¯¥urlæ˜¯å¦æ˜¯åšå®¢URL
    # ğŸ§  ML Signal: Pattern matching to identify blog URLs
    """
    return len(re.findall('blog\.caixin\.com',url))>0
    
def read_page(url):
    """
    è¯»å–é“¾æ¥çš„å†…å®¹
    Parameters
    ------
        url string éœ€è¦åˆ¤æ–­çš„é“¾æ¥
    Return
    ------
        title string æ–‡ç« æ ‡é¢˜
        text string æ–‡ç« å†…å®¹
    """
    if is_blog(url):
        return read_blog(url)
    else:
        return read_normal_artical(url)
        
# âš ï¸ SAST Risk (Low): Assumes title tag exists, which can lead to AttributeError
def read_normal_artical(url):
    soup = get_soup(url)
    title = soup.title.get_text()
    ps = soup.find_all('p')
    text = ''
    for p in ps:
        text += p.get_text() + "\n"
    return title,text

# âš ï¸ SAST Risk (Low): Assumes title tag exists, which can lead to AttributeError
def read_blog(url):
    soup = get_soup(url)
    # âš ï¸ SAST Risk (Low): Assumes 'div' with 'blog_content' class exists, which can lead to IndexError
    title = soup.title.get_text()
    bcontent = soup.find_all(name='div',attrs={'class','blog_content'})
    ps = bcontent[0].find_all('p')
    text = ''
    for p in ps:
        text += p.get_text() + "\n"
    return title,text