[
  {
    "line": 8,
    "text": "from zvt.domain import StockNews, Stock, LimitUpInfo",
    "annotation": "\u2705 Best Practice: Use of __name__ in getLogger for module-specific logging",
    "confidence": 1.0,
    "tokens": [
      6738,
      1976,
      36540,
      13,
      27830,
      1330,
      10500,
      9980,
      11,
      10500,
      11,
      27272,
      4933,
      12360
    ],
    "start_token": 0,
    "end_token": 14,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      286,
      11593,
      3672,
      834,
      287,
      651,
      11187,
      1362,
      329,
      8265,
      12,
      11423,
      18931
    ],
    "label": "best_practice",
    "reason": "Use of __name__ in getLogger for module-specific logging"
  },
  {
    "line": 8,
    "text": "from zvt.domain import StockNews, Stock, LimitUpInfo",
    "annotation": "\u26a0\ufe0f SAST Risk (Medium): Potential file path traversal vulnerability if the file path is influenced by user input",
    "confidence": 0.5,
    "tokens": [
      6738,
      1976,
      36540,
      13,
      27830,
      1330,
      10500,
      9980,
      11,
      10500,
      11,
      27272,
      4933,
      12360
    ],
    "start_token": 14,
    "end_token": 28,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      31205,
      2599,
      32480,
      2393,
      3108,
      33038,
      282,
      15131,
      611,
      262,
      2393,
      3108,
      318,
      12824,
      416,
      2836,
      5128
    ],
    "label": "sast_risk",
    "severity": "Medium",
    "reason": "Potential file path traversal vulnerability if the file path is influenced by user input"
  },
  {
    "line": 9,
    "text": "from zvt.utils.time_utils import date_time_by_interval, today",
    "annotation": "\u2705 Best Practice: Use 'with' statement for file operations to ensure proper resource management",
    "confidence": 1.0,
    "tokens": [
      6738,
      1976,
      36540,
      13,
      26791,
      13,
      2435,
      62,
      26791,
      1330,
      3128,
      62,
      2435,
      62,
      1525,
      62,
      3849,
      2100,
      11,
      1909
    ],
    "start_token": 28,
    "end_token": 48,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      705,
      4480,
      6,
      2643,
      329,
      2393,
      4560,
      284,
      4155,
      1774,
      8271,
      4542
    ],
    "label": "best_practice",
    "reason": "Use 'with' statement for file operations to ensure proper resource management"
  },
  {
    "line": 10,
    "text": "",
    "annotation": "\ud83e\udde0 ML Signal: Usage of os.path to construct file paths",
    "confidence": 0.5,
    "tokens": [],
    "start_token": 48,
    "end_token": 48,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      29566,
      286,
      28686,
      13,
      6978,
      284,
      5678,
      2393,
      13532
    ],
    "label": "ml_signal",
    "reason": "Usage of os.path to construct file paths"
  },
  {
    "line": 12,
    "text": "",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): No error handling for file operations, which may lead to unhandled exceptions",
    "confidence": 0.5,
    "tokens": [],
    "start_token": 48,
    "end_token": 48,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      1400,
      4049,
      9041,
      329,
      2393,
      4560,
      11,
      543,
      743,
      1085,
      284,
      555,
      38788,
      13269
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "No error handling for file operations, which may lead to unhandled exceptions"
  },
  {
    "line": 13,
    "text": "",
    "annotation": "\ud83e\udde0 ML Signal: Usage of json.load to parse JSON files",
    "confidence": 1.0,
    "tokens": [],
    "start_token": 48,
    "end_token": 48,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      29566,
      286,
      33918,
      13,
      2220,
      284,
      21136,
      19449,
      3696
    ],
    "label": "ml_signal",
    "reason": "Usage of json.load to parse JSON files"
  },
  {
    "line": 11,
    "text": "logger = logging.getLogger(__name__)",
    "annotation": "\u2705 Best Practice: Consider adding a docstring to describe the function's purpose and parameters.",
    "confidence": 1.0,
    "tokens": [
      6404,
      1362,
      796,
      18931,
      13,
      1136,
      11187,
      1362,
      7,
      834,
      3672,
      834,
      8
    ],
    "start_token": 48,
    "end_token": 61,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      12642,
      4375,
      257,
      2205,
      8841,
      284,
      6901,
      262,
      2163,
      338,
      4007,
      290,
      10007,
      13
    ],
    "label": "best_practice",
    "reason": "Consider adding a docstring to describe the function's purpose and parameters."
  },
  {
    "line": 13,
    "text": "",
    "annotation": "\u2705 Best Practice: Use consistent casing for text processing to avoid case sensitivity issues.",
    "confidence": 1.0,
    "tokens": [],
    "start_token": 61,
    "end_token": 61,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      6414,
      39731,
      329,
      2420,
      7587,
      284,
      3368,
      1339,
      14233,
      2428,
      13
    ],
    "label": "best_practice",
    "reason": "Use consistent casing for text processing to avoid case sensitivity issues."
  },
  {
    "line": 15,
    "text": "    with open(os.path.join(os.path.dirname(__file__), \"hot.json\")) as f:",
    "annotation": "\u26a0\ufe0f SAST Risk (Low): Ensure get_hot_words_config() is defined and returns expected data structure.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      351,
      1280,
      7,
      418,
      13,
      6978,
      13,
      22179,
      7,
      418,
      13,
      6978,
      13,
      15908,
      3672,
      7,
      834,
      7753,
      834,
      828,
      366,
      8940,
      13,
      17752,
      48774,
      355,
      277,
      25
    ],
    "start_token": 61,
    "end_token": 92,
    "annotation_tokens": [
      158,
      248,
      254,
      37929,
      311,
      11262,
      19602,
      357,
      20535,
      2599,
      48987,
      651,
      62,
      8940,
      62,
      10879,
      62,
      11250,
      3419,
      318,
      5447,
      290,
      5860,
      2938,
      1366,
      4645,
      13
    ],
    "label": "sast_risk",
    "severity": "Low",
    "reason": "Ensure get_hot_words_config() is defined and returns expected data structure."
  },
  {
    "line": 17,
    "text": "",
    "annotation": "\u2705 Best Practice: Initialize dictionaries before use to ensure they are empty.",
    "confidence": 1.0,
    "tokens": [],
    "start_token": 92,
    "end_token": 92,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      20768,
      1096,
      48589,
      3166,
      878,
      779,
      284,
      4155,
      484,
      389,
      6565,
      13
    ],
    "label": "best_practice",
    "reason": "Initialize dictionaries before use to ensure they are empty."
  },
  {
    "line": 20,
    "text": "    text = text.upper()",
    "annotation": "\ud83e\udde0 ML Signal: Iterating over a configuration dictionary to process text data.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      2420,
      796,
      2420,
      13,
      45828,
      3419
    ],
    "start_token": 92,
    "end_token": 101,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      40806,
      803,
      625,
      257,
      8398,
      22155,
      284,
      1429,
      2420,
      1366,
      13
    ],
    "label": "ml_signal",
    "reason": "Iterating over a configuration dictionary to process text data."
  },
  {
    "line": 23,
    "text": "    topic_stats = {}",
    "annotation": "\ud83e\udde0 ML Signal: Counting occurrences of words in text data.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      7243,
      62,
      34242,
      796,
      23884
    ],
    "start_token": 101,
    "end_token": 109,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      2764,
      278,
      40279,
      286,
      2456,
      287,
      2420,
      1366,
      13
    ],
    "label": "ml_signal",
    "reason": "Counting occurrences of words in text data."
  },
  {
    "line": 25,
    "text": "        topic_count = 0",
    "annotation": "\u2705 Best Practice: Use text.lower() consistently to ensure case-insensitive counting.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      7243,
      62,
      9127,
      796,
      657
    ],
    "start_token": 109,
    "end_token": 121,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      2420,
      13,
      21037,
      3419,
      9835,
      284,
      4155,
      1339,
      12,
      1040,
      18464,
      14143,
      13
    ],
    "label": "best_practice",
    "reason": "Use text.lower() consistently to ensure case-insensitive counting."
  },
  {
    "line": 28,
    "text": "            topic_count = topic_count + word_stats[word]",
    "annotation": "\u2705 Best Practice: Use += for incrementing variables for readability.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      7243,
      62,
      9127,
      796,
      7243,
      62,
      9127,
      1343,
      1573,
      62,
      34242,
      58,
      4775,
      60
    ],
    "start_token": 121,
    "end_token": 146,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      15853,
      329,
      18703,
      278,
      9633,
      329,
      1100,
      1799,
      13
    ],
    "label": "best_practice",
    "reason": "Use += for incrementing variables for readability."
  },
  {
    "line": 28,
    "text": "            topic_count = topic_count + word_stats[word]",
    "annotation": "\ud83e\udde0 ML Signal: Aggregating word counts by topic.",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      7243,
      62,
      9127,
      796,
      7243,
      62,
      9127,
      1343,
      1573,
      62,
      34242,
      58,
      4775,
      60
    ],
    "start_token": 146,
    "end_token": 171,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      19015,
      2301,
      803,
      1573,
      9853,
      416,
      7243,
      13
    ],
    "label": "ml_signal",
    "reason": "Aggregating word counts by topic."
  },
  {
    "line": 28,
    "text": "            topic_count = topic_count + word_stats[word]",
    "annotation": "\u2705 Best Practice: Return both topic and word statistics for comprehensive results.",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      7243,
      62,
      9127,
      796,
      7243,
      62,
      9127,
      1343,
      1573,
      62,
      34242,
      58,
      4775,
      60
    ],
    "start_token": 171,
    "end_token": 196,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      8229,
      1111,
      7243,
      290,
      1573,
      7869,
      329,
      9815,
      2482,
      13
    ],
    "label": "best_practice",
    "reason": "Return both topic and word statistics for comprehensive results."
  },
  {
    "line": 23,
    "text": "    topic_stats = {}",
    "annotation": "\ud83e\udde0 ML Signal: Function definition with a parameter type hint indicating expected input data type",
    "confidence": 1.0,
    "tokens": [
      220,
      220,
      220,
      7243,
      62,
      34242,
      796,
      23884
    ],
    "start_token": 196,
    "end_token": 204,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      15553,
      6770,
      351,
      257,
      11507,
      2099,
      9254,
      12739,
      2938,
      5128,
      1366,
      2099
    ],
    "label": "ml_signal",
    "reason": "Function definition with a parameter type hint indicating expected input data type"
  },
  {
    "line": 24,
    "text": "    for topic in hot_words_config:",
    "annotation": "\u2705 Best Practice: Use of type hints for function parameters improves code readability and maintainability",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      329,
      7243,
      287,
      3024,
      62,
      10879,
      62,
      11250,
      25
    ],
    "start_token": 204,
    "end_token": 216,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      286,
      2099,
      20269,
      329,
      2163,
      10007,
      19575,
      2438,
      1100,
      1799,
      290,
      5529,
      1799
    ],
    "label": "best_practice",
    "reason": "Use of type hints for function parameters improves code readability and maintainability"
  },
  {
    "line": 93,
    "text": "                        (f\"{stock_map[entity_id]['code']}({stock_map[entity_id]['name']})\", count)",
    "annotation": "\ud83e\udde0 ML Signal: Function signature with multiple optional parameters",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      357,
      69,
      1,
      90,
      13578,
      62,
      8899,
      58,
      26858,
      62,
      312,
      7131,
      6,
      8189,
      20520,
      92,
      15090,
      13578,
      62,
      8899,
      58,
      26858,
      62,
      312,
      7131,
      6,
      3672,
      20520,
      30072,
      1600,
      954,
      8
    ],
    "start_token": 216,
    "end_token": 271,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      15553,
      9877,
      351,
      3294,
      11902,
      10007
    ],
    "label": "ml_signal",
    "reason": "Function signature with multiple optional parameters"
  },
  {
    "line": 96,
    "text": "        if not is_hot:",
    "annotation": "\ud83e\udde0 ML Signal: Function call with keyword arguments",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      611,
      407,
      318,
      62,
      8940,
      25
    ],
    "start_token": 271,
    "end_token": 284,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      15553,
      869,
      351,
      21179,
      7159
    ],
    "label": "ml_signal",
    "reason": "Function call with keyword arguments"
  },
  {
    "line": 109,
    "text": "            if words in topic_words",
    "annotation": "\u2705 Best Practice: Use of f-string for string formatting",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      611,
      2456,
      287,
      7243,
      62,
      10879
    ],
    "start_token": 284,
    "end_token": 301,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      286,
      277,
      12,
      8841,
      329,
      4731,
      33313
    ],
    "label": "best_practice",
    "reason": "Use of f-string for string formatting"
  },
  {
    "line": 112,
    "text": "",
    "annotation": "\u2705 Best Practice: Use of f-string for string formatting",
    "confidence": 0.5,
    "tokens": [],
    "start_token": 301,
    "end_token": 301,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      286,
      277,
      12,
      8841,
      329,
      4731,
      33313
    ],
    "label": "best_practice",
    "reason": "Use of f-string for string formatting"
  },
  {
    "line": 114,
    "text": "",
    "annotation": "\u2705 Best Practice: List comprehension for concise list creation",
    "confidence": 1.0,
    "tokens": [],
    "start_token": 301,
    "end_token": 301,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      7343,
      35915,
      329,
      35327,
      1351,
      6282
    ],
    "label": "best_practice",
    "reason": "List comprehension for concise list creation"
  },
  {
    "line": 116,
    "text": "",
    "annotation": "\u2705 Best Practice: Use of join for efficient string concatenation",
    "confidence": 1.0,
    "tokens": [],
    "start_token": 301,
    "end_token": 301,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      286,
      4654,
      329,
      6942,
      4731,
      1673,
      36686,
      341
    ],
    "label": "best_practice",
    "reason": "Use of join for efficient string concatenation"
  },
  {
    "line": 114,
    "text": "",
    "annotation": "\u2705 Best Practice: Use of default parameters for function flexibility",
    "confidence": 0.5,
    "tokens": [],
    "start_token": 301,
    "end_token": 301,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      5765,
      286,
      4277,
      10007,
      329,
      2163,
      13688
    ],
    "label": "best_practice",
    "reason": "Use of default parameters for function flexibility"
  },
  {
    "line": 116,
    "text": "",
    "annotation": "\u2705 Best Practice: Handling default value for start_timestamp",
    "confidence": 0.5,
    "tokens": [],
    "start_token": 301,
    "end_token": 301,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      49500,
      4277,
      1988,
      329,
      923,
      62,
      16514,
      27823
    ],
    "label": "best_practice",
    "reason": "Handling default value for start_timestamp"
  },
  {
    "line": 118,
    "text": "def msg_group_stocks_by_topic(",
    "annotation": "\ud83e\udde0 ML Signal: Querying data based on a timestamp, which could be used for time-series analysis",
    "confidence": 0.5,
    "tokens": [
      4299,
      31456,
      62,
      8094,
      62,
      29522,
      62,
      1525,
      62,
      26652,
      7
    ],
    "start_token": 301,
    "end_token": 312,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      2264,
      263,
      1112,
      1366,
      1912,
      319,
      257,
      41033,
      11,
      543,
      714,
      307,
      973,
      329,
      640,
      12,
      25076,
      3781
    ],
    "label": "ml_signal",
    "reason": "Querying data based on a timestamp, which could be used for time-series analysis"
  },
  {
    "line": 120,
    "text": "):",
    "annotation": "\ud83e\udde0 ML Signal: Splitting strings into lists, indicating text processing",
    "confidence": 0.5,
    "tokens": [
      2599
    ],
    "start_token": 312,
    "end_token": 313,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      13341,
      2535,
      13042,
      656,
      8341,
      11,
      12739,
      2420,
      7587
    ],
    "label": "ml_signal",
    "reason": "Splitting strings into lists, indicating text processing"
  },
  {
    "line": 122,
    "text": "        keyword=keyword,",
    "annotation": "\ud83e\udde0 ML Signal: Converting lists to a flat list, useful for frequency analysis",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      21179,
      28,
      2539,
      4775,
      11
    ],
    "start_token": 313,
    "end_token": 325,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      35602,
      889,
      8341,
      284,
      257,
      6228,
      1351,
      11,
      4465,
      329,
      8373,
      3781
    ],
    "label": "ml_signal",
    "reason": "Converting lists to a flat list, useful for frequency analysis"
  },
  {
    "line": 125,
    "text": "        start_timestamp=start_timestamp,",
    "annotation": "\ud83e\udde0 ML Signal: Using pandas Series for frequency counting",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      923,
      62,
      16514,
      27823,
      28,
      9688,
      62,
      16514,
      27823,
      11
    ],
    "start_token": 325,
    "end_token": 342,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      8554,
      19798,
      292,
      7171,
      329,
      8373,
      14143
    ],
    "label": "ml_signal",
    "reason": "Using pandas Series for frequency counting"
  },
  {
    "line": 125,
    "text": "        start_timestamp=start_timestamp,",
    "annotation": "\ud83e\udde0 ML Signal: Counting occurrences of each item, useful for identifying popular topics",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      923,
      62,
      16514,
      27823,
      28,
      9688,
      62,
      16514,
      27823,
      11
    ],
    "start_token": 342,
    "end_token": 359,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      2764,
      278,
      40279,
      286,
      1123,
      2378,
      11,
      4465,
      329,
      13720,
      2968,
      10233
    ],
    "label": "ml_signal",
    "reason": "Counting occurrences of each item, useful for identifying popular topics"
  },
  {
    "line": 125,
    "text": "        start_timestamp=start_timestamp,",
    "annotation": "\u2705 Best Practice: Limiting the result set for performance and relevance",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      923,
      62,
      16514,
      27823,
      28,
      9688,
      62,
      16514,
      27823,
      11
    ],
    "start_token": 359,
    "end_token": 376,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      7576,
      1780,
      262,
      1255,
      900,
      329,
      2854,
      290,
      23082
    ],
    "label": "best_practice",
    "reason": "Limiting the result set for performance and relevance"
  },
  {
    "line": 125,
    "text": "        start_timestamp=start_timestamp,",
    "annotation": "\u2705 Best Practice: Standard Python idiom for making a script both importable and executable",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      923,
      62,
      16514,
      27823,
      28,
      9688,
      62,
      16514,
      27823,
      11
    ],
    "start_token": 376,
    "end_token": 393,
    "annotation_tokens": [
      26486,
      227,
      6705,
      19939,
      25,
      8997,
      11361,
      4686,
      29005,
      329,
      1642,
      257,
      4226,
      1111,
      1330,
      540,
      290,
      28883
    ],
    "label": "best_practice",
    "reason": "Standard Python idiom for making a script both importable and executable"
  },
  {
    "line": 125,
    "text": "        start_timestamp=start_timestamp,",
    "annotation": "\ud83e\udde0 ML Signal: Function call with specific parameters, indicating usage pattern",
    "confidence": 0.5,
    "tokens": [
      220,
      220,
      220,
      220,
      220,
      220,
      220,
      923,
      62,
      16514,
      27823,
      28,
      9688,
      62,
      16514,
      27823,
      11
    ],
    "start_token": 393,
    "end_token": 410,
    "annotation_tokens": [
      8582,
      100,
      254,
      10373,
      26484,
      25,
      15553,
      869,
      351,
      2176,
      10007,
      11,
      12739,
      8748,
      3912
    ],
    "label": "ml_signal",
    "reason": "Function call with specific parameters, indicating usage pattern"
  }
]