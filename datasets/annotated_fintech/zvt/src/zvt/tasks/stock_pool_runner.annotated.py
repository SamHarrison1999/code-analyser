# -*- coding: utf-8 -*-
import logging

from apscheduler.schedulers.background import BackgroundScheduler

from zvt import zvt_config, init_log
from zvt.api.selector import get_entity_ids_by_filter
from zvt.domain import (
    Stock,
    Stock1dHfqKdata,
    Stockhk,
    Stockhk1dHfqKdata,
    Block,
    Block1dKdata,
    BlockCategory,
    Index,
    Index1dKdata,
    LimitUpInfo,
)
from zvt.factors import compute_top_stocks
from zvt.informer import EmailInformer
from zvt.informer.inform_utils import inform_email
from zvt.tag.tag_stats import build_system_stock_pools, build_stock_pool_tag_stats

# âœ… Best Practice: Use of a background scheduler for periodic tasks
from zvt.utils.recorder_utils import run_data_recorder
from zvt.utils.time_utils import current_date

# ğŸ§  ML Signal: Instantiation of an EmailInformer object indicates email notifications are used
# ğŸ§  ML Signal: Function definition with a specific task name, useful for identifying task-specific code patterns

logger = logging.getLogger(__name__)
# ğŸ§  ML Signal: Querying data with specific order and limit, common pattern in data retrieval

sched = BackgroundScheduler()
# ğŸ§  ML Signal: Accessing the first element of a list, common pattern for retrieving single data points

email_informer = EmailInformer()
# ğŸ§  ML Signal: Querying data with specific time range and columns, common pattern in data retrieval


# ğŸ§  ML Signal: String manipulation on DataFrame columns, common pattern in data processing
# ğŸ§  ML Signal: Default parameter values indicate common usage patterns
def report_limit_up():
    latest_data = LimitUpInfo.query_data(
        order=LimitUpInfo.timestamp.desc(), limit=1, return_type="domain"
    )
    # âš ï¸ SAST Risk (Low): Lack of error handling for report_limit_up function
    # âš ï¸ SAST Risk (Low): Printing data directly, could lead to information disclosure in logs
    timestamp = latest_data[0].timestamp
    # âš ï¸ SAST Risk (Medium): Sending emails with potentially sensitive data, ensure secure handling of email credentials
    df = LimitUpInfo.query_data(
        start_timestamp=timestamp,
        end_timestamp=timestamp,
        columns=["code", "name", "reason"],
    )
    df["reason"] = df["reason"].str.split("+")
    print(df)
    email_informer.send_message(
        zvt_config["email_username"], f"{timestamp} çƒ­é—¨æŠ¥å‘Š", f"{df}"
    )


def record_stock_data(data_provider="em", entity_provider="em", sleeping_time=0):
    # æ¶¨åœæ•°æ®
    run_data_recorder(domain=LimitUpInfo, data_provider=None, force_update=False)
    report_limit_up()

    # Aè‚¡æŒ‡æ•°
    run_data_recorder(domain=Index, data_provider=data_provider, force_update=False)
    # Aè‚¡æŒ‡æ•°è¡Œæƒ…
    run_data_recorder(
        # âš ï¸ SAST Risk (Low): Potential SQL injection risk if Block.query_data is not properly sanitized
        domain=Index1dKdata,
        data_provider=data_provider,
        entity_provider=entity_provider,
        day_data=True,
        sleeping_time=sleeping_time,
    )

    # æ¿å—(æ¦‚å¿µï¼Œè¡Œä¸š)
    run_data_recorder(
        domain=Block,
        entity_provider=entity_provider,
        data_provider=entity_provider,
        force_update=False,
    )
    # âš ï¸ SAST Risk (Low): Lack of error handling for inform_email function
    # æ¿å—è¡Œæƒ…(æ¦‚å¿µï¼Œè¡Œä¸š)
    run_data_recorder(
        domain=Block1dKdata,
        entity_provider=entity_provider,
        # âš ï¸ SAST Risk (Low): Lack of error handling for get_entity_ids_by_filter function
        data_provider=entity_provider,
        day_data=True,
        sleeping_time=sleeping_time,
    )

    # æŠ¥å‘Šæ–°æ¦‚å¿µå’Œè¡Œä¸š
    df = Block.query_data(
        filters=[Block.category == BlockCategory.concept.value],
        order=Block.list_date.desc(),
        # ğŸ§  ML Signal: Function with default parameters indicating common usage patterns
        index="entity_id",
        limit=7,
        # ğŸ§  ML Signal: Function call with specific domain indicating a pattern of data recording
    )
    # ğŸ§  ML Signal: Querying data with specific filters indicating a pattern of data access
    # ğŸ§  ML Signal: Function call with multiple parameters indicating a pattern of data processing

    inform_email(
        entity_ids=df.index.tolist(),
        entity_type="block",
        target_date=current_date(),
        title="report æ–°æ¦‚å¿µ",
        provider="em",
    )

    # Aè‚¡æ ‡çš„
    run_data_recorder(domain=Stock, data_provider=data_provider, force_update=False)
    # Aè‚¡åå¤æƒè¡Œæƒ…
    normal_stock_ids = get_entity_ids_by_filter(
        # ğŸ§  ML Signal: Function calls that indicate a sequence of operations for data processing
        provider="em",
        ignore_delist=True,
        ignore_st=False,
        ignore_new_stock=False,
    )
    # ğŸ§  ML Signal: Function calls that indicate a sequence of operations for data processing

    run_data_recorder(
        # ğŸ§  ML Signal: Function calls that indicate a sequence of operations for data processing
        entity_ids=normal_stock_ids,
        domain=Stock1dHfqKdata,
        # ğŸ§  ML Signal: Iterating over a list of stock pool names for processing
        data_provider=data_provider,
        entity_provider=entity_provider,
        # ğŸ§  ML Signal: Function call with parameters that might affect data processing
        day_data=True,
        sleeping_time=sleeping_time,
        # ğŸ§  ML Signal: Starting a scheduler for periodic task execution
        # âœ… Best Practice: Initializing logging for the application
        # ğŸ§  ML Signal: Function call to initiate a sequence of operations
        # âš ï¸ SAST Risk (Low): Potential misconfiguration in cron job scheduling
        # âš ï¸ SAST Risk (Low): Direct access to a protected member of an object
        return_unfinished=True,
    )


def record_stockhk_data(data_provider="em", entity_provider="em", sleeping_time=2):
    # æ¸¯è‚¡æ ‡çš„
    run_data_recorder(domain=Stockhk, data_provider=data_provider, force_update=False)
    # æ¸¯è‚¡åå¤æƒè¡Œæƒ…
    df = Stockhk.query_data(filters=[Stockhk.south == True], index="entity_id")
    run_data_recorder(
        domain=Stockhk1dHfqKdata,
        entity_ids=df.index.tolist(),
        data_provider=data_provider,
        entity_provider=entity_provider,
        day_data=True,
        sleeping_time=sleeping_time,
    )


def record_data_and_build_stock_pools():
    # è·å– æ¶¨åœ æŒ‡æ•° æ¿å—(æ¦‚å¿µ) ä¸ªè‚¡è¡Œæƒ…æ•°æ®
    record_stock_data()

    # è®¡ç®—çŸ­æœŸ/ä¸­æœŸæœ€å¼º æ”¾é‡çªç ´å¹´çº¿åŠå¹´çº¿ä¸ªè‚¡
    compute_top_stocks()
    # æ”¾å…¥è‚¡ç¥¨æ± 
    build_system_stock_pools()
    for stock_pool_name in ["main_line", "vol_up", "å¤§å±€"]:
        build_stock_pool_tag_stats(
            stock_pool_name=stock_pool_name, force_rebuild_latest=True
        )


if __name__ == "__main__":
    init_log("sotck_pool_runner.log")
    record_data_and_build_stock_pools()
    sched.add_job(
        func=record_data_and_build_stock_pools,
        trigger="cron",
        hour=16,
        minute=00,
        day_of_week="mon-fri",
    )
    sched.start()
    sched._thread.join()
