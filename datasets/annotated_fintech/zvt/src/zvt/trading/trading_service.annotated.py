# -*- coding: utf-8 -*-
import logging
from typing import List
# ðŸ§  ML Signal: Usage of external library for pagination

import pandas as pd
from fastapi_pagination.ext.sqlalchemy import paginate

import zvt.api.kdata as kdata_api
import zvt.contract.api as contract_api
from zvt.common.query_models import TimeUnit
from zvt.domain import Stock, StockQuote, Stock1mQuote
from zvt.tag.tag_schemas import StockTags, StockPools
from zvt.trading.common import ExecutionStatus
from zvt.trading.trading_models import (
    BuildTradingPlanModel,
    QueryTradingPlanModel,
    QueryTagQuoteModel,
    QueryStockQuoteModel,
    BuildQueryStockQuoteSettingModel,
    KdataRequestModel,
    TSRequestModel,
# ðŸ§  ML Signal: Usage of utility function for checking DataFrame null values
)
from zvt.trading.trading_schemas import TradingPlan, QueryStockQuoteSetting, TagQuoteStats
from zvt.utils.pd_utils import pd_is_not_null
from zvt.utils.time_utils import (
    to_time_str,
    to_pd_timestamp,
    now_pd_timestamp,
    date_time_by_interval,
    current_date,
    # ðŸ§  ML Signal: Function signature and parameter types can be used to infer usage patterns.
    date_and_time,
# âœ… Best Practice: Use of logging for tracking and debugging
# ðŸ§  ML Signal: API call pattern can be used to understand data retrieval methods.
)

logger = logging.getLogger(__name__)


def query_kdata(kdata_request_model: KdataRequestModel):
    kdata_df = kdata_api.get_kdata(
        entity_ids=kdata_request_model.entity_ids,
        provider=kdata_request_model.data_provider,
        # âœ… Best Practice: Check for null data before processing to avoid errors.
        # âœ… Best Practice: Convert timestamps to a consistent format for easier processing.
        start_timestamp=kdata_request_model.start_timestamp,
        end_timestamp=kdata_request_model.end_timestamp,
        adjust_type=kdata_request_model.adjust_type,
    )
    if pd_is_not_null(kdata_df):
        kdata_df["timestamp"] = kdata_df["timestamp"].apply(lambda x: int(x.timestamp()))
        # âœ… Best Practice: Use apply for row-wise operations to maintain readability.
        kdata_df["data"] = kdata_df.apply(
            lambda x: x[
                ["timestamp", "open", "high", "low", "close", "volume", "turnover", "change_pct", "turnover_rate"]
            ].values.tolist(),
            axis=1,
        )
        # âœ… Best Practice: Use groupby and agg for efficient data aggregation.
        df = kdata_df.groupby("entity_id").agg(
            code=("code", "first"),
            name=("name", "first"),
            # ðŸ§  ML Signal: Usage of external API to fetch trading dates
            level=("level", "first"),
            # âœ… Best Practice: Reset index after groupby to maintain DataFrame structure.
            # ðŸ§  ML Signal: Querying data based on entity IDs and provider
            datas=("data", lambda data: list(data)),
        )
        df = df.reset_index(drop=False)
        return df.to_dict(orient="records")

# ðŸ§  ML Signal: Returning data as a dictionary can indicate data serialization patterns.

# âš ï¸ SAST Risk (Low): Potential risk if ts_df is None or not a DataFrame
# âœ… Best Practice: Using apply with lambda for row-wise operations
def query_ts(ts_request_model: TSRequestModel):
    trading_dates = kdata_api.get_recent_trade_dates(days_count=ts_request_model.days_count)
    ts_df = Stock1mQuote.query_data(
        entity_ids=ts_request_model.entity_ids,
        provider=ts_request_model.data_provider,
        start_timestamp=trading_dates[0],
    )
    if pd_is_not_null(ts_df):
        ts_df["data"] = ts_df.apply(
            lambda x: x[
                ["time", "price", "avg_price", "change_pct", "volume", "turnover", "turnover_rate"]
            # âœ… Best Practice: Using groupby and agg for data aggregation
            ].values.tolist(),
            axis=1,
        )
        # âœ… Best Practice: Use of context manager for session ensures proper resource management
        df = ts_df.groupby("entity_id").agg(
            code=("code", "first"),
            # âœ… Best Practice: Resetting index for a clean DataFrame
            name=("name", "first"),
            # ðŸ§  ML Signal: Conversion of date to string and timestamp for consistent date handling
            datas=("data", lambda data: list(data)),
        # ðŸ§  ML Signal: Converting DataFrame to dictionary for record-oriented data
        )
        df = df.reset_index(drop=False)
        # ðŸ§  ML Signal: Use of trading signal type as a value for plan identification
        return df.to_dict(orient="records")


# ðŸ§  ML Signal: Unique plan ID generation pattern
def build_trading_plan(build_trading_plan_model: BuildTradingPlanModel):
    # âœ… Best Practice: Use of query_data method for data retrieval
    with contract_api.DBSession(provider="zvt", data_schema=TradingPlan)() as session:
        stock_id = build_trading_plan_model.stock_id
        trading_date_str = to_time_str(build_trading_plan_model.trading_date)
        trading_date = to_pd_timestamp(trading_date_str)
        signal = build_trading_plan_model.trading_signal_type.value
        # âœ… Best Practice: Use of query_data method for data retrieval
        # âš ï¸ SAST Risk (Low): Assertion can raise exceptions if condition is not met
        # âœ… Best Practice: Use of ORM model for data manipulation
        plan_id = f"{stock_id}_{trading_date_str}_{signal}"

        datas = TradingPlan.query_data(
            session=session, filters=[TradingPlan.id == plan_id], limit=1, return_type="domain"
        )
        if datas:
            assert len(datas) == 1
            plan = datas[0]
        else:
            datas = Stock.query_data(provider="em", entity_id=stock_id, return_type="domain")
            stock = datas[0]
            plan = TradingPlan(
                id=plan_id,
                entity_id=stock_id,
                stock_id=stock_id,
                stock_code=stock.code,
                stock_name=stock.name,
                trading_date=trading_date,
                expected_open_pct=build_trading_plan_model.expected_open_pct,
                # ðŸ§  ML Signal: Function definition with specific input type can be used to infer usage patterns
                buy_price=build_trading_plan_model.buy_price,
                sell_price=build_trading_plan_model.sell_price,
                # âš ï¸ SAST Risk (Low): Use of external session management, ensure proper handling of session lifecycle
                trading_reason=build_trading_plan_model.trading_reason,
                # ðŸ§  ML Signal: Timestamping for tracking plan creation or update time
                trading_signal_type=signal,
                # âœ… Best Practice: Adding and committing changes to the session
                # ðŸ§  ML Signal: Conditional logic based on object attributes can indicate decision-making patterns
                status=ExecutionStatus.init.value,
            )
        plan.timestamp = now_pd_timestamp()
        # âœ… Best Practice: Refreshing the session to get the latest state of the object
        # ðŸ§  ML Signal: Use of date and time manipulation functions can indicate temporal data handling
        session.add(plan)
        session.commit()
        session.refresh(plan)
        return plan


def query_trading_plan(query_trading_plan_model: QueryTradingPlanModel):
    with contract_api.DBSession(provider="zvt", data_schema=TradingPlan)() as session:
        # ðŸ§  ML Signal: Querying data with specific parameters can indicate data access patterns
        # ðŸ§  ML Signal: Use of a database session pattern
        time_range = query_trading_plan_model.time_range
        if time_range.relative_time_range:
            # ðŸ§  ML Signal: Use of pagination function can indicate handling of large datasets
            # ðŸ§  ML Signal: Querying data with specific filters and ordering
            # âš ï¸ SAST Risk (Low): Potential for SQL injection if filters are not properly sanitized
            start_timestamp = date_time_by_interval(
                current_date(), time_range.relative_time_range.interval, time_range.relative_time_range.time_unit
            )
            end_timestamp = None
        else:
            start_timestamp = time_range.absolute_time_range.start_timestamp
            # ðŸ§  ML Signal: Use of a database session pattern
            end_timestamp = time_range.absolute_time_range.end_timestamp
        # âœ… Best Practice: Use of context manager for database session
        selectable = TradingPlan.query_data(
            # ðŸ§  ML Signal: Querying data with specific filters and order
            # âš ï¸ SAST Risk (Low): Potential SQL injection if filters are not properly sanitized
            session=session, start_timestamp=start_timestamp, end_timestamp=end_timestamp, return_type="select"
        )
        return paginate(session, selectable)


def get_current_trading_plan():
    with contract_api.DBSession(provider="zvt", data_schema=TradingPlan)() as session:
        # âœ… Best Practice: Using a context manager for session management ensures that resources are properly managed and released.
        return TradingPlan.query_data(
            # ðŸ§  ML Signal: Querying data with specific filters and ordering can indicate patterns in data retrieval.
            session=session,
            filters=[TradingPlan.status == ExecutionStatus.pending.value],
            order=TradingPlan.trading_date.asc(),
            return_type="domain",
        )

# ðŸ§  ML Signal: Ordering by trading date can indicate a preference for processing data in chronological order.
# ðŸ§  ML Signal: Filtering by status and trading date can be used to identify specific trading plan conditions.

# ðŸ§  ML Signal: Usage of a specific filter range for change_pct
def get_future_trading_plan():
    # ðŸ§  ML Signal: Returning data as a specific type can indicate how the data is intended to be used.
    with contract_api.DBSession(provider="zvt", data_schema=TradingPlan)() as session:
        return TradingPlan.query_data(
            session=session,
            filters=[TradingPlan.status == ExecutionStatus.init.value],
            order=TradingPlan.trading_date.asc(),
            # ðŸ§  ML Signal: Calculation of statistics from a DataFrame
            # âœ… Best Practice: Logging the current plans can help in debugging and tracking the flow of data.
            return_type="domain",
        )


def check_trading_plan():
    with contract_api.DBSession(provider="zvt", data_schema=TradingPlan)() as session:
        plans = TradingPlan.query_data(
            session=session,
            # âœ… Best Practice: Check if the list is not empty before accessing the first element
            filters=[TradingPlan.status == ExecutionStatus.init.value, TradingPlan.trading_date == current_date()],
            order=TradingPlan.trading_date.asc(),
            return_type="domain",
        )

        logger.debug(f"current plans:{plans}")
# ðŸ§  ML Signal: Usage of a specific filter range for change_pct


def query_quote_stats():
    quote_df = StockQuote.query_data(
        return_type="df",
        filters=[StockQuote.change_pct >= -0.31, StockQuote.change_pct <= 0.31],
        columns=["timestamp", "entity_id", "time", "change_pct", "turnover", "is_limit_up", "is_limit_down"],
    )
    current_stats = cal_quote_stats(quote_df)
    # âœ… Best Practice: Check for null DataFrame before processing
    start_timestamp = current_stats["timestamp"]

    # ðŸ§  ML Signal: Calculation of statistics from a DataFrame
    pre_date_df = Stock1mQuote.query_data(
        # ðŸ§  ML Signal: Usage of DataFrame operations to calculate statistics
        filters=[Stock1mQuote.timestamp < to_time_str(start_timestamp)],
        # âœ… Best Practice: Avoid using magic numbers; consider using a named constant for clarity
        order=Stock1mQuote.timestamp.desc(),
        # ðŸ§  ML Signal: Grouping data by a constant to aggregate over the entire DataFrame
        # ðŸ§  ML Signal: Custom aggregation function to count positive changes
        limit=1,
        columns=["timestamp"],
    )
    pre_date = pre_date_df["timestamp"].tolist()[0]

    if start_timestamp.hour >= 15:
        start_timestamp = date_and_time(pre_date, "15:00")
    else:
        start_timestamp = date_and_time(pre_date, f"{start_timestamp.hour}:{start_timestamp.minute}")
    end_timestamp = date_time_by_interval(start_timestamp, 1, TimeUnit.minute)

    pre_df = Stock1mQuote.query_data(
        return_type="df",
        start_timestamp=start_timestamp,
        # ðŸ§  ML Signal: Custom aggregation function to count non-positive changes
        # ðŸ§  ML Signal: Summing turnover values
        # ðŸ§  ML Signal: Calculating mean of percentage changes
        end_timestamp=end_timestamp,
        # ðŸ§  ML Signal: Counting occurrences of limit up events
        # ðŸ§  ML Signal: Function name suggests a specific task related to stock data processing
        filters=[Stock1mQuote.change_pct >= -0.31, Stock1mQuote.change_pct <= 0.31],
        # ðŸ§  ML Signal: Custom aggregation function to count limit down events
        # ðŸ§  ML Signal: Querying data from a database using specific filters
        columns=["timestamp", "entity_id", "time", "change_pct", "turnover", "is_limit_up", "is_limit_down"],
    )

    if pd_is_not_null(pre_df):
        pre_stats = cal_quote_stats(pre_df)
        current_stats["pre_turnover"] = pre_stats["turnover"]
        # ðŸ§  ML Signal: Converting DataFrame to dictionary for record-based access
        current_stats["turnover_change"] = current_stats["turnover"] - current_stats["pre_turnover"]
    return current_stats


# ðŸ§  ML Signal: Querying data with specific columns and filters
def cal_quote_stats(quote_df):
    quote_df["ss"] = 1

    df = (
        quote_df.groupby("ss")
        .agg(
            timestamp=("timestamp", "last"),
            time=("time", "last"),
            up_count=("change_pct", lambda x: (x > 0).sum()),
            down_count=("change_pct", lambda x: (x <= 0).sum()),
            # ðŸ§  ML Signal: Converting a DataFrame column to a list
            turnover=("turnover", "sum"),
            # ðŸ§  ML Signal: Concatenating DataFrames
            # ðŸ§  ML Signal: Querying data with specific return type and index
            # âš ï¸ SAST Risk (Low): Assumes 'timestamp' column exists and has at least one entry
            change_pct=("change_pct", "mean"),
            limit_up_count=("is_limit_up", "sum"),
            limit_down_count=("is_limit_down", lambda x: (x == True).sum()),
        )
        .reset_index(drop=True)
    )

    return df.to_dict(orient="records")[0]


def cal_tag_quote_stats(stock_pool_name):
    stock_pools: List[StockPools] = StockPools.query_data(
        filters=[StockPools.stock_pool_name == stock_pool_name],
        # ðŸ§  ML Signal: Grouping and aggregating DataFrame data
        order=StockPools.timestamp.desc(),
        limit=1,
        return_type="domain",
    )
    if stock_pools:
        entity_ids = stock_pools[0].entity_ids
    else:
        entity_ids = None
    # ðŸ§  ML Signal: Adding new columns to DataFrame

    # ðŸ§  ML Signal: Applying a function to DataFrame rows
    tag_df = StockTags.query_data(
        entity_ids=entity_ids,
        filters=[StockTags.main_tag.isnot(None)],
        columns=[StockTags.entity_id, StockTags.main_tag],
        # ðŸ§  ML Signal: Creating unique identifiers for DataFrame rows
        # ðŸ§  ML Signal: Usage of a specific query pattern to retrieve stock pool data
        # âš ï¸ SAST Risk (Low): Potential risk if `query_data` method is vulnerable to injection attacks
        return_type="df",
        index="entity_id",
    )

    entity_ids = tag_df["entity_id"].tolist()

    # âœ… Best Practice: Using descending order to get the latest timestamp
    # âš ï¸ SAST Risk (Low): Printing potentially sensitive data to the console
    quote_df = StockQuote.query_data(entity_ids=entity_ids, return_type="df", index="entity_id")
    # ðŸ§  ML Signal: Storing DataFrame to a database
    timestamp = quote_df["timestamp"].tolist()[0]

    df = pd.concat([tag_df, quote_df], axis=1)
    # ðŸ§  ML Signal: Filtering data based on main tags
    grouped_df = (
        df.groupby("main_tag")
        .agg(
            up_count=("change_pct", lambda x: (x > 0).sum()),
            down_count=("change_pct", lambda x: (x <= 0).sum()),
            turnover=("turnover", "sum"),
            change_pct=("change_pct", "mean"),
            # âš ï¸ SAST Risk (Low): Potential risk if `in_` method is vulnerable to injection attacks
            limit_up_count=("is_limit_up", "sum"),
            limit_down_count=("is_limit_down", lambda x: (x == True).sum()),
            total_count=("main_tag", "size"),  # æ·»åŠ è®¡æ•°ï¼Œè®¡ç®—æ¯ä¸ªåˆ†ç»„çš„æ€»è¡Œæ•°
        # âœ… Best Practice: Concatenating DataFrames for combined analysis
        # ðŸ§  ML Signal: Conversion of DataFrame column to list
        # ðŸ§  ML Signal: Querying stock quotes based on entity IDs
        )
        .reset_index(drop=False)
    )
    grouped_df["stock_pool_name"] = stock_pool_name

    grouped_df["entity_id"] = grouped_df[["stock_pool_name", "main_tag"]].apply(
        lambda se: "{}_{}".format(se["stock_pool_name"], se["main_tag"]), axis=1
    )
    grouped_df["timestamp"] = timestamp
    grouped_df["id"] = grouped_df[["entity_id", "timestamp"]].apply(
        lambda se: "{}_{}".format(se["entity_id"], to_time_str(se["timestamp"])), axis=1
    )

    # ðŸ§  ML Signal: Grouping and aggregating data for analysis
    # ðŸ§  ML Signal: Counting positive changes
    print(grouped_df)
    # ðŸ§  ML Signal: Counting non-positive changes

    contract_api.df_to_db(
        # ðŸ§  ML Signal: Summing turnover values
        df=grouped_df, data_schema=TagQuoteStats, provider="zvt", force_update=True, drop_duplicates=False
    )
# ðŸ§  ML Signal: Usage of query pattern with filters and ordering
# ðŸ§  ML Signal: Calculating mean percentage change
# ðŸ§  ML Signal: Counting limit up occurrences


def query_tag_quotes(query_tag_quote_model: QueryTagQuoteModel):
    stock_pools: List[StockPools] = StockPools.query_data(
        filters=[StockPools.stock_pool_name == query_tag_quote_model.stock_pool_name],
        order=StockPools.timestamp.desc(),
        # ðŸ§  ML Signal: Counting limit down occurrences
        limit=1,
        return_type="domain",
    )
    if stock_pools:
        entity_ids = stock_pools[0].entity_ids
    # âœ… Best Practice: Sorting DataFrame for prioritized analysis
    # ðŸ§  ML Signal: Converting DataFrame to dictionary for output
    # ðŸ§  ML Signal: Conditional query based on main_tag presence
    else:
        entity_ids = None

    tag_df = StockTags.query_data(
        entity_ids=entity_ids,
        filters=[StockTags.main_tag.in_(query_tag_quote_model.main_tags)],
        columns=[StockTags.entity_id, StockTags.main_tag],
        return_type="df",
        index="entity_id",
    )

    entity_ids = tag_df["entity_id"].tolist()
    # ðŸ§  ML Signal: Query without filters

    quote_df = StockQuote.query_data(entity_ids=entity_ids, return_type="df", index="entity_id")

    # ðŸ§  ML Signal: Mapping entity_ids to tags
    df = pd.concat([tag_df, quote_df], axis=1)
    grouped_df = (
        # âš ï¸ SAST Risk (High): Use of eval() with dynamic input can lead to code injection
        df.groupby("main_tag")
        .agg(
            # ðŸ§  ML Signal: Query pattern with dynamic ordering
            # ðŸ§  ML Signal: Accessing dictionary keys to retrieve values
            up_count=("change_pct", lambda x: (x > 0).sum()),
            down_count=("change_pct", lambda x: (x <= 0).sum()),
            # ðŸ§  ML Signal: Accessing dictionary keys to retrieve values
            turnover=("turnover", "sum"),
            change_pct=("change_pct", "mean"),
            # ðŸ§  ML Signal: Accessing dictionary keys to retrieve values
            limit_up_count=("is_limit_up", "sum"),
            limit_down_count=("is_limit_down", lambda x: (x == True).sum()),
            total_count=("main_tag", "size"),  # æ·»åŠ è®¡æ•°ï¼Œè®¡ç®—æ¯ä¸ªåˆ†ç»„çš„æ€»è¡Œæ•°
        # ðŸ§  ML Signal: Conditional logic based on dictionary content
        )
        .reset_index(drop=False)
    )
    sorted_df = grouped_df.sort_values(by=["turnover", "total_count"], ascending=[False, False])
    # ðŸ§  ML Signal: Returning a pandas Series from a function

    return sorted_df.to_dict(orient="records")
# ðŸ§  ML Signal: Applying a function across DataFrame rows


# ðŸ§  ML Signal: Counting occurrences based on a condition
def query_stock_quotes(query_stock_quote_model: QueryStockQuoteModel):
    # ðŸ§  ML Signal: Counting occurrences based on a condition
    # ðŸ§  ML Signal: Summing a DataFrame column
    entity_ids = None
    if query_stock_quote_model.stock_pool_name:
        stock_pools: List[StockPools] = StockPools.query_data(
            filters=[StockPools.stock_pool_name == query_stock_quote_model.stock_pool_name],
            order=StockPools.timestamp.desc(),
            limit=1,
            return_type="domain",
        )
        if stock_pools:
            # ðŸ§  ML Signal: Calculating the mean of a DataFrame column
            # ðŸ§  ML Signal: Counting occurrences based on a condition
            entity_ids = stock_pools[0].entity_ids
    # ðŸ§  ML Signal: Function definition with a financial context, useful for identifying domain-specific functions
    else:
        # ðŸ§  ML Signal: Converting DataFrame to a list of dictionaries
        entity_ids = query_stock_quote_model.entity_ids
    # âœ… Best Practice: Define a function to encapsulate stock selling logic for reusability and clarity

    # ðŸ§  ML Signal: Constructing a dictionary with computed values
    if query_stock_quote_model.main_tag:
        # âœ… Best Practice: Use 'pass' as a placeholder for future implementation
        # ðŸ§  ML Signal: Function definition with a specific model parameter indicates a pattern for ML model usage
        tags_dict = StockTags.query_data(
            entity_ids=entity_ids,
            # âš ï¸ SAST Risk (Low): Hardcoded ID value could lead to potential issues if not managed properly
            filters=[StockTags.main_tag == query_stock_quote_model.main_tag],
            return_type="dict",
        )
        # âš ï¸ SAST Risk (Medium): Querying data without input validation can lead to SQL injection risks
        if not tags_dict:
            return None
        # âš ï¸ SAST Risk (Low): Potential risk of large data exposure if quotes is too large
        entity_ids = [item["entity_id"] for item in tags_dict]
    else:
        tags_dict = StockTags.query_data(
            # ðŸ§  ML Signal: Returning a dictionary from a function
            # âœ… Best Practice: Initialize object with default values to ensure consistency
            return_type="dict",
        )
    # ðŸ§  ML Signal: Updating timestamp indicates a pattern of tracking changes over time

    entity_tags_map = {item["entity_id"]: item for item in tags_dict}
    # ðŸ§  ML Signal: Assigning model attributes to object properties shows a pattern of data transformation

    order = eval(f"StockQuote.{query_stock_quote_model.order_by_field}.{query_stock_quote_model.order_by_type.value}()")
    # ðŸ§  ML Signal: Function to build default settings if not present

    # âš ï¸ SAST Risk (Low): Adding objects to session without validation can lead to data integrity issues
    df = StockQuote.query_data(order=order, entity_ids=entity_ids, return_type="df")

    # âš ï¸ SAST Risk (Medium): Committing session without exception handling can lead to unhandled errors
    if not pd_is_not_null(df):
        # ðŸ§  ML Signal: Function call with specific parameters
        return None
    # âœ… Best Practice: Refreshing session to ensure the object is updated with the latest database state

    # ðŸ§  ML Signal: Returning the updated object indicates a pattern of function output for further processing
    # âœ… Best Practice: Use of __name__ guard for script execution
    # âœ… Best Practice: Explicitly defining __all__ for module exports
    def set_tags(quote):
        entity_id = quote["entity_id"]
        main_tag = entity_tags_map.get(entity_id, {}).get("main_tag", None)
        sub_tag = entity_tags_map.get(entity_id, {}).get("sub_tag", None)
        active_hidden_tags = entity_tags_map.get(entity_id, {}).get("active_hidden_tags", None)
        if active_hidden_tags:
            hidden_tags = list(active_hidden_tags.keys())
        else:
            hidden_tags = None
        return pd.Series({"main_tag": main_tag, "sub_tag": sub_tag, "hidden_tags": hidden_tags})

    df[["main_tag", "sub_tag", "hidden_tags"]] = df.apply(set_tags, axis=1)

    up_count = (df["change_pct"] > 0).sum()
    down_count = (df["change_pct"] < 0).sum()
    turnover = df["turnover"].sum()
    change_pct = df["change_pct"].mean()
    limit_up_count = df["is_limit_up"].sum()
    limit_down_count = df["is_limit_down"].sum()

    quotes = df.to_dict(orient="records")

    result = {
        "up_count": up_count,
        "down_count": down_count,
        "turnover": turnover,
        "change_pct": change_pct,
        "limit_up_count": limit_up_count,
        "limit_down_count": limit_down_count,
        "quotes": quotes[: query_stock_quote_model.limit],
    }
    return result


def buy_stocks():
    pass


def sell_stocks():
    pass


def build_query_stock_quote_setting(build_query_stock_quote_setting_model: BuildQueryStockQuoteSettingModel):
    with contract_api.DBSession(provider="zvt", data_schema=QueryStockQuoteSetting)() as session:
        the_id = "admin_setting"
        datas = QueryStockQuoteSetting.query_data(ids=[the_id], session=session, return_type="domain")
        if datas:
            query_setting = datas[0]
        else:
            query_setting = QueryStockQuoteSetting(entity_id="admin", id=the_id)
        query_setting.timestamp = current_date()
        query_setting.stock_pool_name = build_query_stock_quote_setting_model.stock_pool_name
        query_setting.main_tags = build_query_stock_quote_setting_model.main_tags
        session.add(query_setting)
        session.commit()
        session.refresh(query_setting)
        return query_setting


def build_default_query_stock_quote_setting():
    datas = QueryStockQuoteSetting.query_data(ids=["admin_setting"], return_type="domain")
    if datas:
        return
    build_query_stock_quote_setting(BuildQueryStockQuoteSettingModel(stock_pool_name="all", main_tags=["æ¶ˆè´¹ç”µå­"]))


if __name__ == "__main__":
    # print(query_tag_quotes(QueryTagQuoteModel(stock_pool_name="all", main_tags=["ä½Žç©ºç»æµŽ", "åŠå¯¼ä½“", "åŒ–å·¥", "æ¶ˆè´¹ç”µå­"])))
    # print(query_stock_quotes(QueryStockQuoteModel(stock_pool_name="all", main_tag="åŠå¯¼ä½“")))
    print(query_quote_stats())
# the __all__ is generated
__all__ = [
    "build_trading_plan",
    "query_trading_plan",
    "get_current_trading_plan",
    "get_future_trading_plan",
    "check_trading_plan",
    "query_stock_quotes",
    "buy_stocks",
    "sell_stocks",
    "build_query_stock_quote_setting",
]